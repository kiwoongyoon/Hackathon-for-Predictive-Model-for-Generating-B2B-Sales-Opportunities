{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888bca62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: category_encoders in ./.local/lib/python3.10/site-packages (2.6.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in ./.local/lib/python3.10/site-packages (from category_encoders) (0.14.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in ./.local/lib/python3.10/site-packages (from category_encoders) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./.local/lib/python3.10/site-packages (from category_encoders) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in ./.local/lib/python3.10/site-packages (from category_encoders) (1.23.5)\n",
      "Requirement already satisfied: patsy>=0.5.1 in ./.local/lib/python3.10/site-packages (from category_encoders) (0.5.6)\n",
      "Requirement already satisfied: pandas>=1.0.5 in ./.local/lib/python3.10/site-packages (from category_encoders) (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.local/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n",
      "Requirement already satisfied: six in ./.local/lib/python3.10/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in ./.local/lib/python3.10/site-packages (from statsmodels>=0.9.0->category_encoders) (23.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in ./.local/lib/python3.10/site-packages (3.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: colorlog in ./.local/lib/python3.10/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from optuna) (4.65.2)\n",
      "Requirement already satisfied: PyYAML in ./.local/lib/python3.10/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.local/lib/python3.10/site-packages (from optuna) (2.0.25)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.local/lib/python3.10/site-packages (from optuna) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in ./.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\n",
      "Requirement already satisfied: Mako in ./.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./.local/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders \n",
    "!pip install optuna \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import re \n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import category_encoders as ce \n",
    "\n",
    "from sklearn.metrics import (    accuracy_score,    confusion_matrix,f1_score,precision_score, recall_score,roc_auc_score)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold,cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE , ADASYN\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,HistGradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier,StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcb611c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0051658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_corp = {\n",
    "    'Austria': ['LGEAG'],    'Czech Republic': ['LGECZ'],    'France': ['LGEFS'],    'Germany': ['LGEDG'],    'Greece': ['LGEHS'],    'Hungary': ['LGEMK'],    'Italy': ['LGEIS'],    'Netherlands': ['LGESC', 'LGEEH', 'LGEBN'],    'Poland': ['LGEWR', 'LGEPL', 'LGEMA'],    'Portugal': ['LGEPT','LGEBT'],\n",
    "    'EUs': ['LGEEB'],    'Romania': ['LGERO'],    'Spain': ['LGEES'],    'Sweden': ['LGENO', 'LGESW'],    'United Kingdom': ['LGEUK'],      'Kazakhstan': ['LGEAK'],    'Russia': ['LGERM', 'LGERI', 'LGERA'],\n",
    "    'Ukraine': ['LGEUR'],    'Latvia': ['LGELV','LGELA'],    'Algeria': ['LGEAS'],\n",
    "    'Egypt': ['LGEEG'],    'Jordan': ['LGELF'],    'Kenya': ['LGESK','LGEEF'],    'Morocco': ['LGEMC'],\n",
    "    'Saudi Arabia': ['LGESJ'],    'Iran':['LGEIR'],     'Israel':['LGEYK'],     'The Republic of South Africa': ['LGESA'],\n",
    "    'Tunisia': ['LGETU'],    'U.A.E': ['LGEOT', 'LGEDF', 'LGEGF', 'LGEME', 'LGEAF'],    'Nigeria': ['LGEAO', 'LGENI'],\n",
    "    'Turkey': ['LGETK', 'LGEAT'],    'Australia': ['LGEAP'],\n",
    "    'China': ['LGEQA', 'LGETL', 'LGECH', 'LGEYT', 'LGETR', 'LGETA', 'LGESY', 'LGESH', 'LGEQH', 'LGEQD', 'LGEPN', 'LGEND', 'LGEKS', 'LGEHZ', 'LGEHN', 'LGEHK'],\n",
    "    'India': ['LGEIL'],    'Indonesia': ['LGEIN'],    'Japan': ['LGEJP'],    'Malaysia': ['LGEML'],    'Philippines': ['LGEPH'],\n",
    "    'Singapore': ['LGESL'],    'Taiwan': ['LGETT'],    'Korea' :['LGEKR'],    'Thailand': ['LGETH'],    'Vietnam': ['LGEVN','LGEVH'],\n",
    "     'Canada': ['LGECI'],    'Mexico': ['LGERS', 'LGEMX', 'LGEMS', 'LGEMM'],    'United States': ['LGEMR', 'LGEUS', 'LGEMU', 'LGEAI'],\n",
    "    'Argentina': ['LGEAG','LGEAR'],    'Brazil': ['LGEBR','LGESP'],    'Chile': ['LGECL'],    'Colombia': ['LGEVZ', 'LGECB'],\n",
    "    'Panama': ['Guatemala', 'LGEPS'],    'Peru': ['LGEPR']}\n",
    "continent_nation={\n",
    "    'Europe':['EUs','Austria', 'Czech Republic' ,'France' ,'Germany', 'Greece' ,'Hungary', 'Italy', 'Netherlands' ,'Poland' ,'Portugal' ,'Romania', 'Spain' ,'Sweden','United Kingdom'], \n",
    "    'Russia and CIS':['Kazakhstan','Russia', 'Ukraine', 'Latvia'],     'Africa and MiddleEast': ['Israel','Iran','Algeria', 'Egypt', 'Jordan', 'Kenya', 'Morocco','Saudi Arabia','The Republic of South Africa','Tunisia', 'U.A.E', 'Nigeria', 'Turkey'], \n",
    "    'Asia':['Korea','Australia','China','India','Indonesia','Japan','Malaysia','Philippines','Singapore','Taiwan','Thailand','Vietnam'], \n",
    "    'NorthAmerica' : ['Canada','Mexico','United States'],    'SouthAmerica' :['Argentina','Brazil','Chile','Colombia','Panama','Peru']\n",
    "    \n",
    "}\n",
    "hemisphere = {\n",
    "    'Northern': ['EUs', 'Austria', 'Czech Republic', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Spain', 'Sweden', 'United Kingdom', 'Kazakhstan', 'Russia', 'Ukraine', 'Latvia', 'Israel', 'Iran', 'Jordan', 'Morocco', 'Saudi Arabia', 'Tunisia', 'Turkey', 'Korea', 'China', 'Japan', 'Taiwan', 'Canada', 'United States', 'Mexico', 'Panama'],\n",
    "    'Southern': ['Algeria', 'Egypt', 'Kenya', 'The Republic of South Africa', 'U.A.E', 'Nigeria', 'Australia', 'India', 'Indonesia', 'Malaysia', 'Philippines', 'Singapore', 'Thailand', 'Vietnam', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Peru']\n",
    "}\n",
    "mapping_dict = {\n",
    "#     \"Toi muon tim hieu thong tin ky thuat, gia ca cua sp de su dung\": \"Product Information\",\n",
    "#     \"tôi cần tham khảo giá và giải pháp từ LG\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Vui lòng báo giá giúp mình sản phẩm đo thân nhiệt Xin cảm ơn\": \"Request for quotation or purchase\",\n",
    "#     \"LED Signage\": \"Product Information\",\n",
    "#     \"Standalone\": \"Product Information\",\n",
    "#     \"for school\": \"Other\",\n",
    "#     \"Not specified\": \"Other\",\n",
    "#     \"Intégrateur historique du George V\": \"Other\",\n",
    "#     \"Solicito apoyo para realizar cotizacion de los dispositivos que ofrecen en la solución One Quick:\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Pantallas Interactivas para Clinicas\": \"Product Information\",\n",
    "#     \"Hotel TV products\": \"Product Information\",\n",
    "#     \"VRF\": \"Product Information\",\n",
    "#     \"Preciso de um monitor médico para radiografia convencional e tomogrtafia.\": \"Sales Inquiry\",\n",
    "    \"others\": \"Other\",\n",
    "    \"Others\": \"Other\",\n",
    "    \"other_\": \"Other\",\n",
    "    \"other\": \"Other\",\n",
    "    \"Etc.\": \"ETC.\",\n",
    "#     \"window facing product\": \"Product Information\",\n",
    "#     \"Digital platform\": \"Product Information\",\n",
    "#     \"(Select ID_Needs)\": \"Other\",\n",
    "#     \"One Quick:Flex\": \"Product Information\",\n",
    "#     \"AIO\": \"Product Information\",\n",
    "#     \"Needs\": \"Other\",\n",
    "#     \"Hospital TV\": \"Product Information\",\n",
    "#     \"i want to know the details about it\": \"Product Information\",\n",
    "#     \"EDUCATIONAL EQUIPMENTS\": \"Product Information\",\n",
    "#     \"TV interactive\": \"Product Information\",\n",
    "#     \"Hola me pueden cotizar 19 pantallas interactivas de 100 pulgadas entregadas en Guayaquil -Ecuador.\": \"Request for quotation or purchase\",\n",
    "#     \"teach\": \"Other\",\n",
    "#     \"Display Textbook and photos\": \"Usage or technical consultation\",\n",
    "#     \"High inch 86 / 98 or 110\": \"Product Information\",\n",
    "#     \"quotation_\": \"Request for quotation or purchase\",\n",
    "#     \"display product\": \"Product Information\",\n",
    "#     \"first Info and pricing\": \"Quotation or Purchase Consultation\",\n",
    "#     \"estoy buscando para Ecuador este producto LG MAGNIT micro LED, para un cliente de 138 pulgadas, con envió marítimo.\": \"Sales Inquiry\",\n",
    "#     \"Evento_SdelEstero\": \"Other\",\n",
    "#     \"probeam precio\": \"Sales Inquiry\",\n",
    "#     \"media inquiry\": \"Sales Inquiry\",\n",
    "#     \"Video Wall\": \"Product Information\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e3974d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 생성 및 전처리 함수 \n",
    "def get_datas():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)\n",
    "    train['is_converted']=np.where(train['is_converted']==True,1,0)\n",
    "    return train, test \n",
    "\n",
    "\n",
    "def delete_cols(data, cols):\n",
    "    data = data.drop(columns=cols)\n",
    "    return data\n",
    "\n",
    "def log_transform(data,cols):\n",
    "    for col in cols :\n",
    "        data[col+'log']=np.log1p(data[col]) \n",
    "    return data \n",
    "\n",
    "\n",
    "def eda_expected_timeline(df):\n",
    "    \n",
    "    def timeline_label(time):\n",
    "    \n",
    "        time = str(time).lower().replace(' ','').replace('_','').replace('/','').replace(',','').replace('~','').replace('&','').replace('-','').replace('.','')\n",
    "        \n",
    "        if time == 'lessthan3months':\n",
    "            result = 'less than 3 months'\n",
    "        elif time == '3months6months':\n",
    "            result = '3 months ~ 6 months'\n",
    "        elif time == '6months9months':\n",
    "            result = '6 months ~ 9 months'\n",
    "        elif time == '9months1year':\n",
    "            result = '9 months ~ 1 year'\n",
    "        elif time == 'morethanayear':\n",
    "            result = 'more than a year'\n",
    "        else:\n",
    "            result = 'aimers_0203'\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    df['expected_timeline'] = df['expected_timeline'].apply(timeline_label)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# inquiry type 전처리하기 \n",
    "def eda_inquiry_type(df):\n",
    "    df['inquiry_type']= df['inquiry_type'].map(mapping_dict).fillna(train['inquiry_type'])\n",
    "    df.loc[df['inquiry_type'].str.contains('Solicito apoyo para realizar', na=False), 'inquiry_type'] = 'Quotation or Purchase Consultation'\n",
    "    df['inquiry_type'] = df['inquiry_type'].str.lower()\n",
    "    replacement = {'/': ' ', '-':' ', '_':' '}\n",
    "    df['inquiry_type'].replace(replacement, regex=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#customer type 처리 \n",
    "def customer_type(data):\n",
    "    data['customer_type']=data['customer_type'].fillna('none') \n",
    "    return data\n",
    "\n",
    "# total_area 변수로 통일\n",
    "def eda_business_area(df):\n",
    "    for col in ['business_area','business_subarea']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].str.replace(\" \", \"\") \n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', \"\") \n",
    "        df[col] = df[col].fillna('nan') \n",
    "    df['total_area'] = df['business_area'].astype(str) + df['business_subarea'].astype(str)\n",
    "    return df \n",
    "\n",
    "# 새로운 국가명, 대륙 열을 만들기 \n",
    "def get_nation_continent(df):\n",
    "    nation_corp_reverse ={v:k for k , values in nation_corp.items() for v in values }\n",
    "    df['nation']=df['response_corporate'].map(nation_corp_reverse)\n",
    "    continent_nation_reverse ={v:k for k , values in continent_nation.items() for v in values }\n",
    "    df['continent']=df['nation'].map(continent_nation_reverse)\n",
    "#     df = df.drop('customer_country',axis=1) \n",
    "    return df \n",
    "\n",
    "#라벨 인코딩 \n",
    "def label_encoding(series: pd.Series) -> pd.Series:\n",
    "    my_dict = {}\n",
    "    series = series.astype(str)\n",
    "    for idx, value in enumerate(sorted(series.unique())):\n",
    "        my_dict[value] = idx\n",
    "    series = series.map(my_dict)\n",
    "    return series\n",
    "\n",
    "# com_reg_ver_win_rate 최빈값으로 채우기 \n",
    "def com_reg_fill(train,test):\n",
    "    train['com_reg_ver_win_rate'] = train['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    test['com_reg_ver_win_rate'] = test['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    return train,test\n",
    "\n",
    "#****************************Feature Engineering*************************************#\n",
    "\n",
    "# area,unit,continent ->comregverwin , fe9 \n",
    "# area,unit ->ver_win_ratio_per_bu\n",
    "# unit, continent -> fe1 \n",
    "# owner ,unit  -> fe3 \n",
    "# nation, job -> fe10 \n",
    "\n",
    "# 회사별 Quotation or Purchase Consultation,Request for Partnership의 횟수\n",
    "# PortugalChina U.A.E  United States Argentina\n",
    "# 회사별 관료직, 혹은 성공률 높은 이들의 빈도 \n",
    "# unit, position 로 그룹화   x \n",
    "# 국가별 지점의 개수  \n",
    "# 국가별 성공률이 0.1이 넘는 국가 1 나머지 0 \n",
    "\n",
    "\n",
    "def fe_1(train,test):\n",
    "    # unit continent으로 엮어서 영업 전환율 살펴보기 -> 'unit_conti_mean'열 새로 생성\n",
    "    # 대륙별로 어느 사업부에 영업 성공율이 높은 지 \n",
    "    se=train.groupby(['business_unit','continent'])['is_converted'].agg(['mean'])\n",
    "    se = se.rename(columns={'mean':'unit_conti_mean'})\n",
    "    train =train.merge(se, on=['business_unit','continent'], how ='left')\n",
    "    test =test.merge(se, on=['business_unit','continent'], how ='left')\n",
    "    return train,test \n",
    "\n",
    "def fe_2(train,test):\n",
    "    # 영업 당담자가 어느 정도로 다양한 회사(customer_idx)을 담당하고 있는 지 \n",
    "    \n",
    "#     count = train.groupby('lead_owner').size().reset_index(name='leadowner_cnt')     \n",
    "#     train = train.merge(count, on='lead_owner', how='left')\n",
    "#     train['leadowner_cnt']= np.log1p(train['leadowner_cnt'])\n",
    "#     test = test.merge(count, on='lead_owner', how= 'left')\n",
    "#     test['leadowner_cnt']=np.log1p(test['leadowner_cnt'])\n",
    "    unique_count = train.groupby('lead_owner')['customer_idx'].nunique().reset_index(name='unique_cusidx_cnt')\n",
    "    train = train.merge(unique_count, on='lead_owner', how='left')\n",
    "    test = test.merge(unique_count,on ='lead_owner',how ='left')\n",
    "    train['unique_cusidx_cnt']= np.log1p(train['unique_cusidx_cnt'])\n",
    "    test['unique_cusidx_cnt']= np.log1p(test['unique_cusidx_cnt'])\n",
    "    \n",
    "    return train, test \n",
    "\n",
    "def fe_3(train,test):\n",
    "    # 영업담당자와 사업부로 영업전환 성공률 살펴보기 -> 어느 사업부를 어느 담당자가 담당해야 성공율이 높나 확인 \n",
    "\n",
    "    se = train.groupby(['lead_owner','business_unit'])['is_converted'].agg(['mean']).rename(columns={'mean': 'owner_unit_mean'})\n",
    "    train = train.merge(se, on=['lead_owner','business_unit'], how='left')\n",
    "    test = test.merge(se, on=['lead_owner','business_unit'],how='left')\n",
    "    return train, test\n",
    "\n",
    "def fe_4(train,test):\n",
    "    # customer_idx가 대기업, 중소기업으로 분류되는 경우 1을 부여 \n",
    "    se = train[train.groupby('customer_idx')['enterprise'].transform('nunique') > 1]\n",
    "    multi_company=list(se['customer_idx'].unique())\n",
    "    train['multi_company']=np.where(train['customer_idx'].isin(multi_company) ,1,0)\n",
    "    test['multi_company']=np.where(test['customer_idx'].isin(multi_company) ,1,0)\n",
    "    return train, test\n",
    "\n",
    "def fe_5(train,test):\n",
    "    # LG지점 , 사업부 , bantsubmit으로 영업 성공율 살펴보기 -> 너무 과적합됨으로 제외 \n",
    "    se = train.groupby(['response_corporate','business_unit','bant_submit'])['is_converted'].agg(['mean']).rename(columns={'mean':'idx_unit_mean'})\n",
    "    train=train.merge(se,on=['response_corporate','business_unit','bant_submit'], how ='left')\n",
    "    test=test.merge(se,on=['response_corporate','business_unit','bant_submit'], how ='left')\n",
    "    return train, test\n",
    "def fe_6(train,test):\n",
    "    # 영업사원, 사업부, bandsubmit 으로 영업 성공율 살펴보기 -> 과적합으로 제외 \n",
    "    se = train.groupby(['lead_owner','business_unit','bant_submit'])['is_converted'].agg(['mean']).rename(columns={'mean':'idx_unit_mean'})\n",
    "    train=train.merge(se,on=['lead_owner','business_unit','bant_submit'], how ='left')\n",
    "    test =test.merge(se,on =['lead_owner','business_unit','bant_submit'], how ='left')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_7(train,test):\n",
    "    # bant submit 제곱하기 -> isconverted와 corr는 더 높지만 성능향상은 없음 \n",
    "    train['bant_submit']=train['bant_submit']*train['bant_submit']\n",
    "    test['bant_submit']=test['bant_submit']*test['bant_submit']\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_8(df):\n",
    "    # 국가별로 북반구와 남반구 특성을 생성하기 \n",
    "    hemisphere_reverse ={v:k for k , values in hemisphere.items() for v in values }\n",
    "    df['hemisphere'] =df['nation'].map(hemisphere_reverse)\n",
    "    return df \n",
    "\n",
    "def fe_9(train,test):\n",
    "    # 대륙별,사업 분야별, 사업부로 영업 성공률 살피기 \n",
    "    se=train.groupby(['business_area','business_unit','continent'])['is_converted'].agg(['mean'])\n",
    "    se = se.rename(columns={'mean':'area_unit_conti_mean'})\n",
    "    train =train.merge(se, on=['business_area','business_unit','continent'], how ='left')\n",
    "    test =test.merge(se, on=['business_area','business_unit','continent'], how ='left')\n",
    "    return train, test\n",
    "\n",
    "def fe_10(train,test):\n",
    "    #국가별, 고객의 직업에 따라서 영업 성공율 살펴보기 \n",
    "    se =train.groupby(['nation','customer_job'])['is_converted'].agg(['mean']).rename(columns={'mean':'nat_job_mean'})\n",
    "    \n",
    "    train =train.merge(se, on =['nation','customer_job'], how = 'left')\n",
    "    train['nat_job_mean']=train['nat_job_mean'].fillna(train['nat_job_mean'].mean())\n",
    "    \n",
    "    test =test.merge(se, on =['nation','customer_job'], how = 'left')\n",
    "    test['nat_job_mean']=test['nat_job_mean'].fillna(train['nat_job_mean'].mean())\n",
    "    return train,test \n",
    "\n",
    "\n",
    "\n",
    "# def fe_11(train,test):\n",
    "#     se1 =train.groupby(['business_unit','business_area'])['is_converted'].agg(['mean']).rename(columns={'mean':'new_perbu'})\n",
    "#     train =train.merge(se1,on=['business_unit','business_area'], how='left')\n",
    "#     test =test.merge(se1,on=['business_unit','business_area'], how='left')\n",
    "#     train = train.drop('ver_win_ratio_per_bu', axis = 1)\n",
    "    \n",
    "#     test = test.drop('ver_win_ratio_per_bu', axis = 1)\n",
    "#     return train,test \n",
    "\n",
    "def fe_12(train,test):\n",
    "    train['com_product'] = train['product_category'].apply(lambda x: 1 if 'signage' in str(x) else 0)\n",
    "    \n",
    "    se= train.groupby(['customer_idx'])['com_product'].agg(['mean']).rename(columns={'mean':'com_prod_mean'})\n",
    "    train = train.merge(se, on =['customer_idx'], how ='left')\n",
    "    test = test.merge(se, on =['customer_idx'], how ='left')\n",
    "    train= train.drop('com_product', axis= 1 )\n",
    "    return train,test\n",
    "    \n",
    "def fe_13(train,test):\n",
    "    se = train.groupby(['nation', 'inquiry_type'])['is_converted'].agg(['mean']).rename(columns={'mean':'nat_inquiry_type_mean'})\n",
    "    train =train.merge(se,on=['nation', 'inquiry_type'],how='left')\n",
    "    train['nat_inquiry_type_mean']=train['nat_inquiry_type_mean'].fillna(train['nat_inquiry_type_mean'].mean())\n",
    "    \n",
    "    test =test.merge(se,on=['nation', 'inquiry_type'],how='left')\n",
    "    test['nat_inquiry_type_mean']=test['nat_inquiry_type_mean'].fillna(train['nat_inquiry_type_mean'].mean())\n",
    "    return train,test\n",
    "\n",
    "def fe_14(train,test):\n",
    "    se =train.groupby(['nation'])['response_corporate'].agg(['count']).rename(columns={'count':'nr_count'})\n",
    "    train = train.merge(se, on='nation', how='left')\n",
    "    test  = test.merge(se,on='nation',how='left')\n",
    "    return train,test \n",
    "\n",
    "\n",
    "def fe_15(train,test):\n",
    "    se =train.groupby(['business_unit','continent','customer_position'])['is_converted'].agg(['mean']).rename(columns={'mean':'unit_conti_pos'})\n",
    "    \n",
    "    train =train.merge(se, on =['business_unit','continent','customer_position'], how = 'left')\n",
    "    train['unit_conti_pos']=train['unit_conti_pos'].fillna(train['unit_conti_pos'].mean())\n",
    "    test =test.merge(se, on =['business_unit','continent','customer_position'], how = 'left')\n",
    "    test['unit_conti_pos']=test['unit_conti_pos'].fillna(train['unit_conti_pos'].mean())\n",
    "    return train,test\n",
    "\n",
    "def fe_16(train,test):\n",
    "    se =train.groupby(['continent','total_area'])['is_converted'].agg(['mean']).rename(columns={'mean':'total_conti_pos'})\n",
    "    train =train.merge(se, on =['continent','total_area'], how = 'left')\n",
    "    train['total_conti_pos']=train['total_conti_pos'].fillna(train['total_conti_pos'].mean())\n",
    "    test =test.merge(se, on =['continent','total_area'], how = 'left')\n",
    "    test['total_conti_pos']=test['total_conti_pos'].fillna(train['total_conti_pos'].mean())\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def fe_17(train,test):\n",
    "    train['good']=0 \n",
    "    test['good'] = 0 \n",
    "    train.loc[train['nation'].isin(['Taiwan', 'Latvia','Czech Republic','China','Romania','Morocco','Portugal','Thailand','Argentina','U.A.E','United States']), 'good'] = 1\n",
    "    test.loc[test['nation'].isin(['Taiwan', 'Latvia','Czech Republic','China','Romania','Morocco','Portugal','Thailand','Argentina','U.A.E','United States']), 'good'] = 1\n",
    "    return train,test \n",
    "\n",
    "def fe_18(train,test, col1,col2):\n",
    "    time_avg = train[[col2, 'is_converted']].groupby(col2).mean()\n",
    "    time_avg.columns = [f'{col2}_avg']\n",
    "\n",
    "    timeline = train.loc[train[col2] != 'weoif', col1 + [col2]]\n",
    "    timeline['cnt'] = 1\n",
    "    timeline_se = timeline.groupby(col1 + [col2]).count()\n",
    "    timeline_se.reset_index(inplace =True)\n",
    "    temp2 = pd.merge(timeline_se, time_avg, how = 'left' , on=[col2])\n",
    "    temp2['multip'] = temp2['cnt'] * temp2[f'{col2}_avg']\n",
    "    temp2 = temp2.groupby(col1).sum().reset_index().drop([f'{col2}_avg'], axis =1)\n",
    "\n",
    "    temp2[f'{col2}_mean'] = temp2['multip'] / temp2['cnt']\n",
    "    temp2.drop(['multip','cnt'], axis=1 , inplace= True)\n",
    "\n",
    "    train= pd.merge(train, temp2, how ='left' , on=col1)\n",
    "    test= pd.merge(test, temp2, how ='left' , on=col1)\n",
    "    return train,test \n",
    "\n",
    "\n",
    "def create_grouped_features(train, test, group, numeric_var):\n",
    "    # 범주형 특성들에 대해서 다른 수치형 데이터의 중앙값, 최대, 합을 새로운 열로 추가하기 \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    aggs = ['median', 'max','sum']\n",
    "    for agg in aggs:\n",
    "        # groupby 후 aggregation\n",
    "        a1 = train.groupby([group])[numeric_var].agg(agg).to_dict()\n",
    "        # 새로운 feature 생성\n",
    "        train[numeric_var+'_'+group+'_'+agg] = train[group].map(a1)\n",
    "        test[numeric_var+'_'+group+'_'+agg] = test[group].map(a1)\n",
    "    return train, test\n",
    "\n",
    "def do_scale(train,test, scale_cols) :\n",
    "    for c in scale_cols:\n",
    "        min_value = train[c].min()\n",
    "        max_value = train[c].max()\n",
    "        train[c+'sc'] = (train[c] - min_value) / (max_value - min_value)\n",
    "        test[c+'sc'] = (test[c] - min_value) / (max_value - min_value)\n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9947b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['business_unit','customer_idx']\n",
    "numeric_vars = ['historical_existing_cnt', 'lead_desc_length']\n",
    "scale_cols = ['com_reg_ver_win_rate','historical_existing_cnt', 'lead_desc_length','ver_win_rate_x'] \n",
    "\n",
    "#ver_win_ratio_per_bu 스케일 하면 1개수 소폭상승 \n",
    "# 스케일로 대체하면 상승 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7406f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data 갖고오기 \n",
    "train,test= get_datas() \n",
    "# 스케일링 하기 \n",
    "train,test =do_scale(train,test,scale_cols)\n",
    "# 범주형 데이터에 대해 수치형 데이터 통계값 추가\n",
    "for group in groups:\n",
    "    for numeric_var in numeric_vars:\n",
    "        train, test = create_grouped_features(train, test, group, numeric_var)\n",
    "        \n",
    "        \n",
    "# 전처리, 로그변환 수행하기 \n",
    "columns_to_log=['com_reg_ver_win_rate','lead_desc_length']\n",
    "train,test= log_transform(train,columns_to_log ),log_transform(test,columns_to_log)\n",
    "train,test =eda_business_area(train),eda_business_area(test)\n",
    "train,test= get_nation_continent(train),get_nation_continent(test)\n",
    "train,test=eda_expected_timeline(train) ,eda_expected_timeline(test)\n",
    "train,test=customer_type(train) ,customer_type(test)\n",
    "train,test=eda_inquiry_type(train) ,eda_inquiry_type(test)\n",
    "\n",
    "# Feature Engineering \n",
    "train,test = fe_1(train,test)\n",
    "train,test = fe_2(train,test)\n",
    "train,test = fe_3(train,test)\n",
    "\n",
    "train,test = fe_9(train,test)\n",
    "# train,test = fe_10(train,test)\n",
    "# train,test = fe_18(train,test, ['nation', 'enterprise'],'customer_position')\n",
    "train,test = fe_18(train,test, ['continent', 'bant_submit'],'inquiry_type')\n",
    "\n",
    "\n",
    "for col in ['customer_idx','customer_type',]:\n",
    "    train[col+'count'] =train[col].map(train[col].value_counts())\n",
    "    test[col+'count'] =test[col].map(train[col].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6fcd1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train['idit_strategic_ver']=train['idit_strategic_ver'].fillna(0)\n",
    "# train['ver_cus']=train['ver_cus'].fillna(0)\n",
    "# train['ver_pro']=train['ver_pro'].fillna(0)\n",
    "# test['idit_strategic_ver']=test['idit_strategic_ver'].fillna(0)\n",
    "# test['ver_cus']=test['ver_cus'].fillna(0)\n",
    "# test['ver_pro']=test['ver_pro'].fillna(0)\n",
    "\n",
    "# # 3이나 4에 대해서 수행한다 \n",
    "# features=['bant_submit', 'lead_desc_length', 'idit_strategic_ver','ver_cus','ver_pro']\n",
    "# kmeans = KMeans(n_clusters=4,n_init=25, max_iter = 600, random_state=42).fit(train[features])\n",
    "# train[\"cluster_no\"] = kmeans.predict(train[features])\n",
    "# test[\"cluster_no\"] = kmeans.predict(test[features])\n",
    "\n",
    "# for var in ['com_reg_ver_win_rate']:\n",
    "    \n",
    "    \n",
    "#     a1 = train.groupby(['cluster_no'])[var].mean().to_dict()\n",
    "#     train['cluster_no_by'+'_'+var+'_'+'median'] = train['cluster_no'].map(a1)\n",
    "#     test['cluster_no_by'+'_'+var+'_'+'median'] = test['cluster_no'].map(a1)\n",
    "\n",
    "#     a1 = train.groupby(['cluster_no'])[var].max().to_dict()\n",
    "#     train['cluster_no_by'+'_'+var+'_'+'max'] = train['cluster_no'].map(a1)\n",
    "#     test['cluster_no_by'+'_'+var+'_'+'max'] = test['cluster_no'].map(a1)\n",
    "    \n",
    "# train=train.drop('cluster_no', axis=1)\n",
    "# test=test.drop('cluster_no', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f91d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# country.1지우지말기 \n",
    "columns_to_delete=['nation',]\n",
    "# columns_to_delete=[]\n",
    "train,test =delete_cols(train, columns_to_delete), delete_cols(test,columns_to_delete)\n",
    "\n",
    "cols = [     'customer_country',    \"business_subarea\",    \"business_area\",    \"business_unit\",    \"customer_type\",    \"enterprise\",    \"customer_job\",    \"inquiry_type\",    \"product_category\",    \"product_subcategory\",    \"product_modelname\",    \"customer_position\",\n",
    "      'customer_country.1', \"response_corporate\",  \n",
    "     \"expected_timeline\",\n",
    "'nation','continent',\n",
    "'total_area',\n",
    "#         'res_unit'\n",
    "#         'idx_unit'\n",
    "# 'hemisphere'`\n",
    "      ]\n",
    "label_columns =list(set(cols)-set(columns_to_delete))\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "enc = CatBoostEncoder(cols=label_columns)\n",
    "enc.fit(train[label_columns], train['is_converted'])  # 'target'은 실제 데이터의 타겟 변수 이름에 맞게 변경\n",
    "# 인코딩 적용\n",
    "train[label_columns] = enc.transform(train[label_columns])\n",
    "test[label_columns] = enc.transform(test[label_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "dd312c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "x = train.drop('is_converted', axis=1)\n",
    "y = train.is_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a181c3",
   "metadata": {},
   "source": [
    "# 모델 생성, 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4ca42af5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.8560099999999998, 정밀도: 0.8543499999999999, 재현율: 0.8579399999999999, ROC-AUC: 0.9235089762384374\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.8   71.1]\n",
      " [  68.9  416.1]]\n",
      "> F1 Score: 0.85555, 정밀도: 0.85395, 재현율: 0.85732, ROC-AUC: 0.9231933031720677\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.7   71.2]\n",
      " [  69.2  415.8]]\n",
      "> F1 Score: 0.8585200000000001, 정밀도: 0.8562099999999999, 재현율: 0.8610399999999998, ROC-AUC: 0.9249438597081368\n",
      "> 평균 검증 오차행렬: \n",
      " [[5374.7   70.2]\n",
      " [  67.4  417.6]]\n",
      "> F1 Score: 0.85684, 정밀도: 0.85244, 재현율: 0.86144, ROC-AUC: 0.9249479510364143\n",
      "> 평균 검증 오차행렬: \n",
      " [[5372.5   72.4]\n",
      " [  67.2  417.8]]\n",
      "> F1 Score: 0.8562999999999998, 정밀도: 0.8544, 재현율: 0.8583500000000001, ROC-AUC: 0.9236342344687738\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.9   71. ]\n",
      " [  68.7  416.3]]\n",
      "0.856644 0.924045664924766 416.71999999999997 5373.719999999999\n",
      "> F1 Score: 0.8553199999999999, 정밀도: 0.8526199999999999, 재현율: 0.8581299999999998, ROC-AUC: 0.9237373018152008\n",
      "> 평균 검증 오차행렬: \n",
      " [[5372.9   72. ]\n",
      " [  68.8  416.2]]\n",
      "Count of 0: 3429\n",
      "Count of 1: 1842\n"
     ]
    }
   ],
   "source": [
    "def dtc_skfold(zero_wei,one_wei,seed) :\n",
    "     #Decisiontree에 대해서만 skfold 적용하는 함수 \n",
    "    real_preds = []\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    model = DecisionTreeClassifier(random_state=seed ,class_weight =class_weight)\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "#     Skfold = StratifiedShuffleSplit(n_splits=10, random_state=42)  \n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    tt = []\n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test)\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴\n",
    "\n",
    "f1_avg,roc_avg,tt=0,0,0\n",
    "avg_get_1 =0\n",
    "for seed in [5,11,30,322,8940]:\n",
    "    _,f1,roc,ones,tts =dtc_skfold(1,1,seed)\n",
    "    f1_avg+= f1 \n",
    "    roc_avg+= roc \n",
    "    avg_get_1+=ones\n",
    "    tt+= tts \n",
    "print(f1_avg/5,roc_avg/5,avg_get_1/5, tt/5)\n",
    "\n",
    "dtc_preds,_,_,_,_= dtc_skfold(1,1,3)\n",
    "\n",
    "predicts_array = np.array(dtc_preds)\n",
    "\n",
    "# axis=0를 기준으로 평균 계산\n",
    "# average_preds = np.mean(predicts_array, axis=0)\n",
    "\n",
    "# average_preds[0]\n",
    "final_prediction = np.mean(predicts_array, axis=0)\n",
    "\n",
    "final_prediction = np.where(final_prediction < 0.1, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(final_prediction == 0))\n",
    "count_2 = np.size(np.where(final_prediction >0))\n",
    "count_1 = np.size(np.where(final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "55c52663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(3)에 대해서만 제출 할 때 \n",
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= final_prediction\n",
    "sub.to_csv('submission.csv',index= False)\n",
    "sub.to_csv('708_fe18(2)-fe10_1842.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "499569c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.85754, 정밀도: 0.85581, 재현율: 0.8595700000000001, ROC-AUC: 0.924390479074915\n",
      "> 평균 검증 오차행렬: \n",
      " [[5374.4   70.5]\n",
      " [  68.1  416.9]]\n",
      "> F1 Score: 0.85695, 정밀도: 0.85269, 재현율: 0.86144, ROC-AUC: 0.9251523243316058\n",
      "> 평균 검증 오차행렬: \n",
      " [[5372.6   72.3]\n",
      " [  67.2  417.8]]\n",
      "> F1 Score: 0.85472, 정밀도: 0.85122, 재현율: 0.8585600000000001, ROC-AUC: 0.923549858133514\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.9   73. ]\n",
      " [  68.6  416.4]]\n",
      "> F1 Score: 0.8564700000000001, 정밀도: 0.85517, 재현율: 0.8579399999999999, ROC-AUC: 0.9237644674041723\n",
      "> 평균 검증 오차행렬: \n",
      " [[5374.3   70.6]\n",
      " [  68.9  416.1]]\n",
      "> F1 Score: 0.8561800000000002, 정밀도: 0.8516600000000001, 재현율: 0.8610300000000001, ROC-AUC: 0.9247850010210653\n",
      "> 평균 검증 오차행렬: \n",
      " [[5372.    72.9]\n",
      " [  67.4  417.6]]\n",
      "> F1 Score: 0.8554599999999999, 정밀도: 0.85056, 재현율: 0.86061, ROC-AUC: 0.9247375430500983\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.4   73.5]\n",
      " [  67.6  417.4]]\n",
      "> F1 Score: 0.8531299999999999, 정밀도: 0.8494800000000001, 재현율: 0.8571099999999999, ROC-AUC: 0.9229493021125164\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.    73.9]\n",
      " [  69.3  415.7]]\n",
      "> F1 Score: 0.8565699999999999, 정밀도: 0.85396, 재현율: 0.8593900000000001, ROC-AUC: 0.9240074008750563\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.5   71.4]\n",
      " [  68.2  416.8]]\n",
      "> F1 Score: 0.8558299999999999, 정밀도: 0.8530899999999999, 재현율: 0.85876, ROC-AUC: 0.9240530878981532\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.    71.9]\n",
      " [  68.5  416.5]]\n",
      "> F1 Score: 0.8572, 정밀도: 0.85534, 재현율: 0.85937, ROC-AUC: 0.924073971262696\n",
      "> 평균 검증 오차행렬: \n",
      " [[5374.2   70.7]\n",
      " [  68.2  416.8]]\n"
     ]
    }
   ],
   "source": [
    "# seed 앙상블 (현재는 1부터 10 )\n",
    "\n",
    "seed_ens = []\n",
    "for seed in range(1,11):\n",
    "    dtc_preds,_,_,_,_= dtc_skfold(1,1,seed)\n",
    "    seed_ens.append(dtc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "642632ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0: 3410\n",
      "Count of 1: 1861\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predicts_array = np.array(seed_ens)\n",
    "average_preds = np.mean(predicts_array, axis=0)\n",
    "average_preds = np.sum(average_preds, axis =0 )\n",
    "final_prediction = np.where(average_preds < 0.4, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(final_prediction == 0))\n",
    "count_1 = np.size(np.where(final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)\n",
    "\n",
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= final_prediction\n",
    "sub.to_csv('submission.csv',index= False)\n",
    "sub.to_csv('708_seed1to10_0.4_1861.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4aee9c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances:\n",
      "owner_unit_mean: 0.3630428763284205\n",
      "lead_desc_length_customer_idx_sum: 0.2210427488938615\n",
      "customer_idx: 0.07064678261060821\n",
      "customer_country.1: 0.06866716843312666\n",
      "customer_idxcount: 0.06096289545693483\n",
      "customer_type: 0.019811568867936606\n",
      "product_category: 0.013944978642024618\n",
      "lead_owner: 0.011747920736627471\n",
      "nat_job_mean: 0.01159846497816395\n",
      "customer_country: 0.010984212413895111\n",
      "historical_existing_cnt_business_unit_sum: 0.00965687462524489\n",
      "lead_desc_length_customer_idx_median: 0.009288958326025918\n",
      "customer_job: 0.009019892359458588\n",
      "lead_desc_length_customer_idx_max: 0.008785899655644223\n",
      "unique_cusidx_cnt: 0.007346352430433808\n",
      "area_unit_conti_mean: 0.006746296467078819\n",
      "business_subarea: 0.006530782446492523\n",
      "nation: 0.006433000409989283\n",
      "total_area: 0.0060293777601502\n",
      "lead_desc_length: 0.005804695148184775\n",
      "customer_position: 0.005238592677206086\n",
      "bant_submit: 0.005125008457184076\n",
      "nat_inquiry_type_mean: 0.005083886005436374\n",
      "expected_timeline: 0.004780168279253842\n",
      "product_modelname: 0.0045750301322341315\n",
      "lead_desc_length_scaled: 0.004285076993101804\n",
      "lead_desc_lengthlog: 0.004180421762392181\n",
      "enterprise: 0.00336922093579739\n",
      "historical_existing_cnt_customer_idx_median: 0.003365439672375306\n",
      "inquiry_type: 0.002953755070344471\n",
      "response_corporate: 0.0029280286716829645\n",
      "business_area: 0.0027386910265191967\n",
      "customer_typecount: 0.0024375192762817934\n",
      "product_subcategory: 0.0024313530739156226\n",
      "continent: 0.002021496922561377\n",
      "historical_existing_cnt_customer_idx_sum: 0.001947662119843262\n",
      "com_reg_ver_win_ratelog: 0.0015621371372178405\n",
      "ver_win_ratio_per_bu: 0.001536887068838208\n",
      "ver_win_rate_x_scaled: 0.0014084735061341545\n",
      "historical_existing_cnt: 0.0013926819284453302\n",
      "com_reg_ver_win_rate_scaled: 0.0013166129920192502\n",
      "unit_conti_mean: 0.0012910989458492011\n",
      "com_reg_ver_win_rate: 0.0011301686883412286\n",
      "historical_existing_cnt_scaled: 0.0009137328169147846\n",
      "ver_win_rate_x: 0.000743225301479512\n",
      "lead_desc_length_business_unit_sum: 0.0005226467463868114\n",
      "ver_pro: 0.0004221231081218588\n",
      "historical_existing_cnt_business_unit_max: 0.00038477160885774277\n",
      "historical_existing_cnt_customer_idx_max: 0.00035594304683682717\n",
      "idit_strategic_ver: 0.0002920619069310698\n",
      "it_strategic_ver: 0.00020254634055984135\n",
      "lead_desc_length_business_unit_median: 0.00019297056371284977\n",
      "ver_cus: 0.00017846736203680513\n",
      "lead_desc_length_business_unit_max: 0.00016884924324874352\n",
      "historical_existing_cnt_business_unit_median: 0.00016884924324874352\n",
      "business_unit: 0.00015008821622110535\n",
      "id_strategic_ver: 0.00011256616216582902\n"
     ]
    }
   ],
   "source": [
    "# DTC 모델 학습\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(x, y)\n",
    "\n",
    "    \n",
    "\n",
    "# 특성 중요도와 특성 이름을 numpy array로 변환\n",
    "importances = np.array(dtc.feature_importances_)\n",
    "feature_names = np.array(x.columns)\n",
    "\n",
    "# 중요도에 따라 내림차순으로 정렬\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# 정렬된 특성 중요도 출력\n",
    "print(\"Feature importances:\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {importances[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fc400ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= final_prediction\n",
    "sub.to_csv('submission.csv',index= False)\n",
    "sub.to_csv('708_4clusterminmax_dropclustno.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c82f5f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1853"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9fdbeb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.85472, 정밀도: 0.85122, 재현율: 0.8585600000000001, ROC-AUC: 0.923549858133514\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.9   73. ]\n",
      " [  68.6  416.4]]\n"
     ]
    }
   ],
   "source": [
    "def SkfoldCV(zero_wei, one_wei, seed, model_name,weight_arr=[1,1,1,1]):\n",
    "    \n",
    "    w1,w2,w3,w4 = weight_arr[0], weight_arr[1], weight_arr[2], weight_arr[3]\n",
    "    real_preds,tt= [],[]\n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "    \n",
    "    if model_name == 'dtc':\n",
    "        model = DecisionTreeClassifier(random_state=seed ,class_weight=class_weight,)\n",
    "    elif model_name == 'rf':\n",
    "        model = RandomForestClassifier(random_state=seed ,class_weight=class_weight,)\n",
    "    elif model_name == 'cat':\n",
    "        model = CatBoostClassifier(random_state=seed ,auto_class_weights='Balanced', silent =True)\n",
    "    elif model_name == 'xgb' :\n",
    "        model = XGBClassifier(random_state=seed ,class_weight=class_weight, )\n",
    "    elif model_name == 'bag' :\n",
    "        model = BaggingClassifier(random_state=seed, )\n",
    "    elif model_name == 'voting':\n",
    "        model1 = DecisionTreeClassifier(random_state=seed ,class_weight=class_weight, )\n",
    "        model3 = CatBoostClassifier(random_state=seed ,auto_class_weights='Balanced', silent =True)\n",
    "        model4 = XGBClassifier(random_state=seed ,class_weight=class_weight, )\n",
    "        model5 = BaggingClassifier(random_state=seed,)\n",
    "        models = [('dtc', model1), ('cat', model3), ('xgb', model4), ('bag', model5)]\n",
    "        weights = [w1,w2,w3,w4]  # 가중치는 모델의 성능에 따라 조절 가능\n",
    "        model = VotingClassifier(estimators=models, voting='soft', weights=weights)\n",
    "    \n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test)\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "#     ###############prediction 1의 개수 #########################\n",
    "#     predicts_array = np.array(voting_preds)\n",
    "#     final_prediction = np.mean(predicts_array, axis=0)\n",
    "#     final_prediction = np.where(final_prediction < 0.1, 0, 1)\n",
    "#     count_1 = np.size(np.where(final_prediction == 1))\n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴\n",
    "\n",
    "\n",
    "\n",
    "# 'dtc', 'cat', 'rf', 'xgb','bag'\n",
    "\n",
    "dtc_preds,_,_,TN,_= SkfoldCV(1,1,3, 'dtc')\n",
    "# cat_preds,_,_,TN,_= SkfoldCV(1,1,3, 'cat')\n",
    "# rf_preds,_,_,TN,_= SkfoldCV(1,1,3, 'rf')\n",
    "# xgb_preds,_,_,TN,_= SkfoldCV(1,1,3, 'xgb')\n",
    "# bag_preds,_,_,TN,_= SkfoldCV(1,1,3, 'bag')\n",
    "# voting_preds,_,_,TN,_,cnt_1= SkfoldCV(1,1,3, 'voting',[4,2,1,1])\n",
    "# print(cnt_1)\n",
    "wei_arrays = [\n",
    "    [7, 3, 2, 2],    [8, 3, 3, 2],    [9, 3, 3, 3],    [8, 4, 2, 2],\n",
    "    [9, 4, 3, 2],    [10, 4, 3, 3],    [9, 5, 2, 2],    [10, 5, 3, 2],\n",
    "    [11, 5, 3, 3],    [10, 6, 2, 2],    [11, 6, 3, 2],    [11, 7, 2, 2],\n",
    "    [10, 4, 4, 2],    [11, 4, 4, 3],    [10, 5, 4, 1],    [11, 5, 4, 2],\n",
    "    [10, 6, 3, 1],    [11, 6, 4, 1],    [10, 7, 2, 1],    [11, 7, 3, 1]\n",
    "]\n",
    "\n",
    "\n",
    "# for wei_arr in wei_arrays:\n",
    "#     print(wei_arr)\n",
    "#     voting_preds,_,_,TN,_,cnt_1= SkfoldCV(1,1,3, 'voting',wei_arr)\n",
    "#     print(cnt_1)\n",
    "#     print(\"------------------------------------\")\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d25c2dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5271"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicts를 numpy array로 변환\n",
    "predicts_array = np.array(predicts)\n",
    "\n",
    "# axis=0를 기준으로 평균 계산\n",
    "average_preds = np.mean(predicts_array, axis=0)\n",
    "\n",
    "# average_preds[0]\n",
    "final_prediction = np.mean(average_preds, axis=0)\n",
    "len(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fb1d453",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 1, 0, ..., 0, 0, 1]),\n",
       " array([1, 1, 0, ..., 0, 0, 1]),\n",
       " array([1, 1, 0, ..., 0, 1, 1]),\n",
       " array([1, 1, 0, ..., 0, 1, 0]),\n",
       " array([0, 1, 0, ..., 0, 0, 1]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 0]),\n",
       " array([0, 1, 0, ..., 0, 1, 0]),\n",
       " array([1, 1, 0, ..., 0, 0, 1]),\n",
       " array([0, 1, 0, ..., 0, 0, 1]),\n",
       " array([0, 1, 0, ..., 0, 0, 1]),\n",
       " array([0, 1, 0, ..., 0, 0, 1]),\n",
       " array([0, 1, 0, ..., 0, 0, 1]),\n",
       " array([0, 1, 0, ..., 0, 0, 1])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds =[3,42,83,400,201193]\n",
    "# preds=[]\n",
    "class_weight= {0:3, 1:1}\n",
    "for seed in seeds :\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=seed, class_weight= class_weight)\n",
    "    model.fit(x,y) \n",
    "    pred= model.predict(test)\n",
    "    preds.append(pred)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2f971e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0: 3437\n",
      "Count of 1: 1834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5271"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = np.array(predicts)\n",
    "# predictions\n",
    "# 예측값들의 평균을 계산하여 최종 예측값 도출\n",
    "# final_prediction = np.mean(average_preds, axis=0)\n",
    "\n",
    "# 0.2 미만은 0으로, 0.2 이상은 1로 변환\n",
    "# predicts를 numpy array로 변환\n",
    "predicts_array = np.array(dtc_preds)\n",
    "\n",
    "# axis=0를 기준으로 평균 계산\n",
    "# average_preds = np.mean(predicts_array, axis=0)\n",
    "\n",
    "# average_preds[0]\n",
    "final_prediction = np.mean(predicts_array, axis=0)\n",
    "\n",
    "final_prediction = np.where(final_prediction < 0.1, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(final_prediction == 0))\n",
    "count_2 = np.size(np.where(final_prediction >0))\n",
    "count_1 = np.size(np.where(final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)\n",
    "len(final_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d9f56",
   "metadata": {},
   "source": [
    "# 모델 제출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "da60aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= final_prediction\n",
    "sub.to_csv('submission.csv',index= False)\n",
    "sub.to_csv('sota2_fe910_idxtypecnt_1834_0.708.csv',index= False)\n",
    "\n",
    "# sub.to_csv('sk10_fe1238timelineinquerycusttype_comregfillcb_2088_seed3_0.1_1:1__submission.csv',index= False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
