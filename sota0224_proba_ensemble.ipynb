{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders \n",
    "# !pip install optuna \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import re \n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import category_encoders as ce \n",
    "\n",
    "from sklearn.metrics import (    accuracy_score,    confusion_matrix,f1_score,precision_score, recall_score,roc_auc_score)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold,cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE , ADASYN\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,HistGradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier,StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "nation_corp = {\n",
    "    'Austria': ['LGEAG'],    'Czech Republic': ['LGECZ'],    'France': ['LGEFS'],    'Germany': ['LGEDG'],    'Greece': ['LGEHS'],    'Hungary': ['LGEMK'],    'Italy': ['LGEIS'],    'Netherlands': ['LGESC', 'LGEEH', 'LGEBN'],    'Poland': ['LGEWR', 'LGEPL', 'LGEMA'],    'Portugal': ['LGEPT','LGEBT'],\n",
    "    'EUs': ['LGEEB'],    'Romania': ['LGERO'],    'Spain': ['LGEES'],    'Sweden': ['LGENO', 'LGESW'],    'United Kingdom': ['LGEUK'],      'Kazakhstan': ['LGEAK'],    'Russia': ['LGERM', 'LGERI', 'LGERA'],\n",
    "    'Ukraine': ['LGEUR'],    'Latvia': ['LGELV','LGELA'],    'Algeria': ['LGEAS'],\n",
    "    'Egypt': ['LGEEG'],    'Jordan': ['LGELF'],    'Kenya': ['LGESK','LGEEF'],    'Morocco': ['LGEMC'],\n",
    "    'Saudi Arabia': ['LGESJ'],    'Iran':['LGEIR'],     'Israel':['LGEYK'],     'The Republic of South Africa': ['LGESA'],\n",
    "    'Tunisia': ['LGETU'],    'U.A.E': ['LGEOT', 'LGEDF', 'LGEGF', 'LGEME', 'LGEAF'],    'Nigeria': ['LGEAO', 'LGENI'],\n",
    "    'Turkey': ['LGETK', 'LGEAT'],    'Australia': ['LGEAP'],\n",
    "    'China': ['LGEQA', 'LGETL', 'LGECH', 'LGEYT', 'LGETR', 'LGETA', 'LGESY', 'LGESH', 'LGEQH', 'LGEQD', 'LGEPN', 'LGEND', 'LGEKS', 'LGEHZ', 'LGEHN', 'LGEHK'],\n",
    "    'India': ['LGEIL'],    'Indonesia': ['LGEIN'],    'Japan': ['LGEJP'],    'Malaysia': ['LGEML'],    'Philippines': ['LGEPH'],\n",
    "    'Singapore': ['LGESL'],    'Taiwan': ['LGETT'],    'Korea' :['LGEKR'],    'Thailand': ['LGETH'],    'Vietnam': ['LGEVN','LGEVH'],\n",
    "     'Canada': ['LGECI'],    'Mexico': ['LGERS', 'LGEMX', 'LGEMS', 'LGEMM'],    'United States': ['LGEMR', 'LGEUS', 'LGEMU', 'LGEAI'],\n",
    "    'Argentina': ['LGEAG','LGEAR'],    'Brazil': ['LGEBR','LGESP'],    'Chile': ['LGECL'],    'Colombia': ['LGEVZ', 'LGECB'],\n",
    "    'Panama': ['Guatemala', 'LGEPS'],    'Peru': ['LGEPR']}\n",
    "continent_nation={\n",
    "    'Europe':['EUs','Austria', 'Czech Republic' ,'France' ,'Germany', 'Greece' ,'Hungary', 'Italy', 'Netherlands' ,'Poland' ,'Portugal' ,'Romania', 'Spain' ,'Sweden','United Kingdom'], \n",
    "    'Russia and CIS':['Kazakhstan','Russia', 'Ukraine', 'Latvia'],     'Africa and MiddleEast': ['Israel','Iran','Algeria', 'Egypt', 'Jordan', 'Kenya', 'Morocco','Saudi Arabia','The Republic of South Africa','Tunisia', 'U.A.E', 'Nigeria', 'Turkey'], \n",
    "    'Asia':['Korea','Australia','China','India','Indonesia','Japan','Malaysia','Philippines','Singapore','Taiwan','Thailand','Vietnam'], \n",
    "    'NorthAmerica' : ['Canada','Mexico','United States'],    'SouthAmerica' :['Argentina','Brazil','Chile','Colombia','Panama','Peru']\n",
    "    \n",
    "}\n",
    "hemisphere = {\n",
    "    'Northern': ['EUs', 'Austria', 'Czech Republic', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Spain', 'Sweden', 'United Kingdom', 'Kazakhstan', 'Russia', 'Ukraine', 'Latvia', 'Israel', 'Iran', 'Jordan', 'Morocco', 'Saudi Arabia', 'Tunisia', 'Turkey', 'Korea', 'China', 'Japan', 'Taiwan', 'Canada', 'United States', 'Mexico', 'Panama'],\n",
    "    'Southern': ['Algeria', 'Egypt', 'Kenya', 'The Republic of South Africa', 'U.A.E', 'Nigeria', 'Australia', 'India', 'Indonesia', 'Malaysia', 'Philippines', 'Singapore', 'Thailand', 'Vietnam', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Peru']\n",
    "}\n",
    "mapping_dict = {\n",
    "#     \"Toi muon tim hieu thong tin ky thuat, gia ca cua sp de su dung\": \"Product Information\",\n",
    "#     \"tôi cần tham khảo giá và giải pháp từ LG\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Vui lòng báo giá giúp mình sản phẩm đo thân nhiệt Xin cảm ơn\": \"Request for quotation or purchase\",\n",
    "#     \"LED Signage\": \"Product Information\",\n",
    "#     \"Standalone\": \"Product Information\",\n",
    "#     \"for school\": \"Other\",\n",
    "#     \"Not specified\": \"Other\",\n",
    "#     \"Intégrateur historique du George V\": \"Other\",\n",
    "#     \"Solicito apoyo para realizar cotizacion de los dispositivos que ofrecen en la solución One Quick:\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Pantallas Interactivas para Clinicas\": \"Product Information\",\n",
    "#     \"Hotel TV products\": \"Product Information\",\n",
    "#     \"VRF\": \"Product Information\",\n",
    "#     \"Preciso de um monitor médico para radiografia convencional e tomogrtafia.\": \"Sales Inquiry\",\n",
    "    \"others\": \"Other\",\n",
    "    \"Others\": \"Other\",\n",
    "    \"other_\": \"Other\",\n",
    "    \"other\": \"Other\",\n",
    "    \"Etc.\": \"ETC.\",\n",
    "#     \"window facing product\": \"Product Information\",\n",
    "#     \"Digital platform\": \"Product Information\",\n",
    "#     \"(Select ID_Needs)\": \"Other\",\n",
    "#     \"One Quick:Flex\": \"Product Information\",\n",
    "#     \"AIO\": \"Product Information\",\n",
    "#     \"Needs\": \"Other\",\n",
    "#     \"Hospital TV\": \"Product Information\",\n",
    "#     \"i want to know the details about it\": \"Product Information\",\n",
    "#     \"EDUCATIONAL EQUIPMENTS\": \"Product Information\",\n",
    "#     \"TV interactive\": \"Product Information\",\n",
    "#     \"Hola me pueden cotizar 19 pantallas interactivas de 100 pulgadas entregadas en Guayaquil -Ecuador.\": \"Request for quotation or purchase\",\n",
    "#     \"teach\": \"Other\",\n",
    "#     \"Display Textbook and photos\": \"Usage or technical consultation\",\n",
    "#     \"High inch 86 / 98 or 110\": \"Product Information\",\n",
    "#     \"quotation_\": \"Request for quotation or purchase\",\n",
    "#     \"display product\": \"Product Information\",\n",
    "#     \"first Info and pricing\": \"Quotation or Purchase Consultation\",\n",
    "#     \"estoy buscando para Ecuador este producto LG MAGNIT micro LED, para un cliente de 138 pulgadas, con envió marítimo.\": \"Sales Inquiry\",\n",
    "#     \"Evento_SdelEstero\": \"Other\",\n",
    "#     \"probeam precio\": \"Sales Inquiry\",\n",
    "#     \"media inquiry\": \"Sales Inquiry\",\n",
    "#     \"Video Wall\": \"Product Information\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성 및 전처리 함수 \n",
    "def get_datas():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)\n",
    "    train['is_converted']=np.where(train['is_converted']==True,1,0)\n",
    "    return train, test \n",
    "\n",
    "\n",
    "def delete_cols(data, cols):\n",
    "    data = data.drop(columns=cols)\n",
    "    return data\n",
    "\n",
    "def log_transform(data,cols):\n",
    "    for col in cols :\n",
    "        data[col+'log']=np.log1p(data[col]) \n",
    "    return data \n",
    "\n",
    "\n",
    "def eda_expected_timeline(df):\n",
    "    \n",
    "    def timeline_label(time):\n",
    "    \n",
    "        time = str(time).lower().replace(' ','').replace('_','').replace('/','').replace(',','').replace('~','').replace('&','').replace('-','').replace('.','')\n",
    "        \n",
    "        if time == 'lessthan3months':\n",
    "            result = 'less than 3 months'\n",
    "        elif time == '3months6months':\n",
    "            result = '3 months ~ 6 months'\n",
    "        elif time == '6months9months':\n",
    "            result = '6 months ~ 9 months'\n",
    "        elif time == '9months1year':\n",
    "            result = '9 months ~ 1 year'\n",
    "        elif time == 'morethanayear':\n",
    "            result = 'more than a year'\n",
    "        else:\n",
    "            result = 'aimers_0203'\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    df['expected_timeline'] = df['expected_timeline'].apply(timeline_label)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# inquiry type 전처리하기 \n",
    "def eda_inquiry_type(df):\n",
    "    df['inquiry_type']= df['inquiry_type'].map(mapping_dict).fillna(train['inquiry_type'])\n",
    "    df.loc[df['inquiry_type'].str.contains('Solicito apoyo para realizar', na=False), 'inquiry_type'] = 'Quotation or Purchase Consultation'\n",
    "    df['inquiry_type'] = df['inquiry_type'].str.lower()\n",
    "    replacement = {'/': ' ', '-':' ', '_':' '}\n",
    "    df['inquiry_type'].replace(replacement, regex=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#customer type 처리 \n",
    "def customer_type(data):\n",
    "    data['customer_type']=data['customer_type'].fillna('none') \n",
    "    return data\n",
    "\n",
    "# total_area 변수로 통일\n",
    "def eda_business_area(df):\n",
    "    for col in ['business_area','business_subarea']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].str.replace(\" \", \"\") \n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', \"\") \n",
    "        df[col] = df[col].fillna('nan') \n",
    "    df['total_area'] = df['business_area'].astype(str) + df['business_subarea'].astype(str)\n",
    "    return df \n",
    "\n",
    "# 새로운 국가명, 대륙 열을 만들기 \n",
    "def get_nation_continent(df):\n",
    "    nation_corp_reverse ={v:k for k , values in nation_corp.items() for v in values }\n",
    "    df['nation']=df['response_corporate'].map(nation_corp_reverse)\n",
    "    continent_nation_reverse ={v:k for k , values in continent_nation.items() for v in values }\n",
    "    df['continent']=df['nation'].map(continent_nation_reverse)\n",
    "#     df = df.drop('customer_country',axis=1) \n",
    "    return df \n",
    "\n",
    "#라벨 인코딩 \n",
    "def label_encoding(series: pd.Series) -> pd.Series:\n",
    "    my_dict = {}\n",
    "    series = series.astype(str)\n",
    "    for idx, value in enumerate(sorted(series.unique())):\n",
    "        my_dict[value] = idx\n",
    "    series = series.map(my_dict)\n",
    "    return series\n",
    "\n",
    "# com_reg_ver_win_rate 최빈값으로 채우기 \n",
    "def com_reg_fill(train,test):\n",
    "    train['com_reg_ver_win_rate'] = train['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    test['com_reg_ver_win_rate'] = test['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    return train,test\n",
    "\n",
    "def timeline_tonumber(row):\n",
    "    if row['expected_timeline'] == 'less than 3 months':\n",
    "        return int(1)\n",
    "    elif row['expected_timeline'] == '3 months ~ 6 months':\n",
    "        return int(3)\n",
    "    elif row['expected_timeline'] == '6 months ~ 9 months':\n",
    "        return int(6)\n",
    "    elif row['expected_timeline'] == '9 months ~ 1 year':\n",
    "        return int(9)    \n",
    "    elif row['expected_timeline'] =='aimer_0203':\n",
    "        return np.nan\n",
    "    else : \n",
    "        return int(12)\n",
    "    \n",
    "def create_grouped_features(train, test, group, numeric_var):\n",
    "    # 범주형 특성들에 대해서 다른 수치형 데이터의 중앙값, 최대, 합을 새로운 열로 추가하기 \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    aggs = ['median', 'max','sum']\n",
    "    for agg in aggs:\n",
    "        # groupby 후 aggregation\n",
    "        a1 = train.groupby([group])[numeric_var].agg(agg).to_dict()\n",
    "        # 새로운 feature 생성\n",
    "        train[numeric_var+'_'+group+'_'+agg] = train[group].map(a1)\n",
    "        test[numeric_var+'_'+group+'_'+agg] = test[group].map(a1)\n",
    "    return train, test\n",
    "\n",
    "def do_scale(train,test, scale_cols) :\n",
    "    for c in scale_cols:\n",
    "        min_value = train[c].min()\n",
    "        max_value = train[c].max()\n",
    "        train[c] = (train[c] - min_value) / (max_value - min_value)\n",
    "        test[c] = (test[c] - min_value) / (max_value - min_value)\n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['business_unit','customer_idx']\n",
    "numeric_vars = ['historical_existing_cnt', 'lead_desc_length']\n",
    "scale_cols = ['com_reg_ver_win_rate','historical_existing_cnt', 'lead_desc_length','ver_win_rate_x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 갖고오기 \n",
    "train,test= get_datas() \n",
    "\n",
    "# 스케일링 하기 \n",
    "train,test =do_scale(train,test,scale_cols)\n",
    "# 범주형 데이터에 대해 수치형 데이터 통계값 추가\n",
    "for group in groups:\n",
    "    for numeric_var in numeric_vars:\n",
    "        train, test = create_grouped_features(train, test, group, numeric_var)\n",
    "        \n",
    "# 전처리, 로그변환 수행하기 \n",
    "columns_to_log=['com_reg_ver_win_rate','lead_desc_length']\n",
    "train,test= log_transform(train,columns_to_log ),log_transform(test,columns_to_log)\n",
    "train,test =eda_business_area(train),eda_business_area(test)\n",
    "train,test= get_nation_continent(train),get_nation_continent(test)\n",
    "train,test=eda_expected_timeline(train) ,eda_expected_timeline(test)\n",
    "train,test=customer_type(train) ,customer_type(test)\n",
    "train,test=eda_inquiry_type(train) ,eda_inquiry_type(test)\n",
    "\n",
    "\n",
    "for col in ['customer_idx','customer_type',]:\n",
    "    train[col+'count'] =train[col].map(train[col].value_counts())\n",
    "    test[col+'count'] =test[col].map(train[col].value_counts())\n",
    "    \n",
    "\n",
    "train['idx_unit'] = train['customer_idx'].astype(str)+train['business_unit'].astype(str)\n",
    "test['idx_unit'] = test['customer_idx'].astype(str)+test['business_unit'].astype(str)\n",
    "train['idx_posi'] = train['customer_idx'].astype(str)+train['customer_position'].astype(str)\n",
    "test['idx_posi'] = test['customer_idx'].astype(str)+test['customer_position'].astype(str)\n",
    "train['conti_inquiry'] = train['continent'].astype(str)+train['inquiry_type'].astype(str)\n",
    "test['conti_inquiry'] = test['continent'].astype(str)+test['inquiry_type'].astype(str)\n",
    "# train['job_unit'] = train['business_unit'].astype(str)+train['customer_job'].astype(str)\n",
    "# test['job_unit'] = test['business_unit'].astype(str)+test['customer_job'].astype(str)\n",
    "#0.717 나옴 4개 변수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country.1지우지말기 \n",
    "columns_to_delete=['customer_country']\n",
    "# columns_to_delete=[]\n",
    "train,test =delete_cols(train, columns_to_delete), delete_cols(test,columns_to_delete)\n",
    "\n",
    "cols = [     'customer_country',    \"business_subarea\",    \"business_area\",    \"business_unit\",    \"customer_type\",    \"enterprise\",    \"customer_job\",    \"inquiry_type\",    \"product_category\",    \"product_subcategory\",    \"product_modelname\",    \"customer_position\",\n",
    "      'customer_country.1', \"response_corporate\", \"expected_timeline\",\n",
    "'nation','continent','lead_owner','idx_posi', 'conti_inquiry', 'idx_unit','bant_submit'\n",
    ",'total_area'   ]\n",
    "label_columns =list(set(cols)-set(columns_to_delete))\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "enc = CatBoostEncoder(cols=label_columns)\n",
    "enc.fit(train[label_columns], train['is_converted'])  # 'target'은 실제 데이터의 타겟 변수 이름에 맞게 변경\n",
    "# 인코딩 적용\n",
    "train[label_columns] = enc.transform(train[label_columns])\n",
    "test[label_columns] = enc.transform(test[label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)\n",
    "train = train.fillna(0)\n",
    "x = train.drop('is_converted', axis=1)\n",
    "y = train.is_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sota : random_state = 42\n",
    "\n",
    "# dtc = DecisionTreeClassifier(random_state=i) \n",
    "# dtc.fit(x,y)\n",
    "\n",
    "# xgb = XGBClassifier(random_state=i)\n",
    "# xgb.fit(x,y)\n",
    "\n",
    "# lgb = LGBMClassifier(random_state=i)\n",
    "# lgb.fit(x,y)\n",
    "\n",
    "# cat = CatBoostClassifier(random_state=i)\n",
    "# cat.fit(x,y)\n",
    "\n",
    "# preds1 = dtc.predict_proba(test)[:,1]\n",
    "# preds2 = xgb.predict_proba(test)[:,1]\n",
    "# preds3 = lgb.predict_proba(test)[:,1]\n",
    "# preds4 = cat.predict_proba(test)[:,1]\n",
    "\n",
    "# preds1 = np.array(preds1).reshape(1,-1)\n",
    "# preds2 = np.array(preds2).reshape(1,-1)\n",
    "# preds3 = np.array(preds3).reshape(1,-1)\n",
    "# preds4 = np.array(preds4).reshape(1,-1)\n",
    "\n",
    "# all_preds = np.concatenate((preds1,preds2),axis=0)\n",
    "# all_preds = np.concatenate((all_preds,preds3),axis=0)\n",
    "# all_preds = np.concatenate((all_preds,preds4),axis=0)\n",
    "\n",
    "# final_preds = np.sum(all_preds,axis=0)\n",
    "\n",
    "# all_preds = np.concatenate((preds1,preds2),axis=0)\n",
    "# all_preds = np.concatenate((all_preds,preds3),axis=0)\n",
    "# all_preds = np.concatenate((all_preds,preds4),axis=0)\n",
    "\n",
    "# final_preds = np.sum(all_preds,axis=0)\n",
    "\n",
    "# final_prediction = np.where(final_preds >= 0.02, 1, 0)\n",
    "# np.sum(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    dtc = DecisionTreeClassifier(random_state=i) \n",
    "    dtc.fit(x,y)\n",
    "\n",
    "    xgb = XGBClassifier(random_state=i)\n",
    "    xgb.fit(x,y)\n",
    "\n",
    "    lgb = LGBMClassifier(random_state=i)\n",
    "    lgb.fit(x,y)\n",
    "\n",
    "    cat = CatBoostClassifier(random_state=i)\n",
    "    cat.fit(x,y)\n",
    "\n",
    "    preds1 = dtc.predict_proba(test)[:,1]\n",
    "    preds2 = xgb.predict_proba(test)[:,1]\n",
    "    preds3 = lgb.predict_proba(test)[:,1]\n",
    "    preds4 = cat.predict_proba(test)[:,1]\n",
    "\n",
    "    preds1 = np.array(preds1).reshape(1,-1)\n",
    "    preds2 = np.array(preds2).reshape(1,-1)\n",
    "    preds3 = np.array(preds3).reshape(1,-1)\n",
    "    preds4 = np.array(preds4).reshape(1,-1)\n",
    "    \n",
    "    all_preds = np.concatenate((preds1,preds2),axis=0)\n",
    "    all_preds = np.concatenate((all_preds,preds3),axis=0)\n",
    "    all_preds = np.concatenate((all_preds,preds4),axis=0)\n",
    "\n",
    "    final_preds = np.sum(all_preds,axis=0)\n",
    "    \n",
    "    ls.append(final_preds)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstacked_array = np.vstack(np.array(ls))\n",
    "\n",
    "seed_preds = np.mean(vstacked_array,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstacked_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = np.where(seed_preds >= 0.02, 1, 0)\n",
    "np.sum(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(3)에 대해서만 제출 할 때 \n",
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= final_prediction\n",
    "sub.to_csv('submission.csv',index= False)\n",
    "sub.to_csv('0224_nofe2_proba002_seed_ensemble.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
