{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inquiry_type & customer type 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import re \n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import category_encoders as ce \n",
    "\n",
    "from sklearn.metrics import (    accuracy_score,    confusion_matrix,f1_score,precision_score, recall_score,roc_auc_score)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold,cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE , ADASYN\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,HistGradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier,StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "nation_corp = {\n",
    "    'Austria': ['LGEAG'],    'Czech Republic': ['LGECZ'],    'France': ['LGEFS'],    'Germany': ['LGEDG'],    'Greece': ['LGEHS'],    'Hungary': ['LGEMK'],    'Italy': ['LGEIS'],    'Netherlands': ['LGESC', 'LGEEH', 'LGEBN'],    'Poland': ['LGEWR', 'LGEPL', 'LGEMA'],    'Portugal': ['LGEPT','LGEBT'],\n",
    "    'EUs': ['LGEEB'],    'Romania': ['LGERO'],    'Spain': ['LGEES'],    'Sweden': ['LGENO', 'LGESW'],    'United Kingdom': ['LGEUK'],      'Kazakhstan': ['LGEAK'],    'Russia': ['LGERM', 'LGERI', 'LGERA'],\n",
    "    'Ukraine': ['LGEUR'],    'Latvia': ['LGELV','LGELA'],    'Algeria': ['LGEAS'],\n",
    "    'Egypt': ['LGEEG'],    'Jordan': ['LGELF'],    'Kenya': ['LGESK','LGEEF'],    'Morocco': ['LGEMC'],\n",
    "    'Saudi Arabia': ['LGESJ'],    'Iran':['LGEIR'],     'Israel':['LGEYK'],     'The Republic of South Africa': ['LGESA'],\n",
    "    'Tunisia': ['LGETU'],    'U.A.E': ['LGEOT', 'LGEDF', 'LGEGF', 'LGEME', 'LGEAF'],    'Nigeria': ['LGEAO', 'LGENI'],\n",
    "    'Turkey': ['LGETK', 'LGEAT'],    'Australia': ['LGEAP'],\n",
    "    'China': ['LGEQA', 'LGETL', 'LGECH', 'LGEYT', 'LGETR', 'LGETA', 'LGESY', 'LGESH', 'LGEQH', 'LGEQD', 'LGEPN', 'LGEND', 'LGEKS', 'LGEHZ', 'LGEHN', 'LGEHK'],\n",
    "    'India': ['LGEIL'],    'Indonesia': ['LGEIN'],    'Japan': ['LGEJP'],    'Malaysia': ['LGEML'],    'Philippines': ['LGEPH'],\n",
    "    'Singapore': ['LGESL'],    'Taiwan': ['LGETT'],    'Korea' :['LGEKR'],    'Thailand': ['LGETH'],    'Vietnam': ['LGEVN','LGEVH'],\n",
    "     'Canada': ['LGECI'],    'Mexico': ['LGERS', 'LGEMX', 'LGEMS', 'LGEMM'],    'United States': ['LGEMR', 'LGEUS', 'LGEMU', 'LGEAI'],\n",
    "    'Argentina': ['LGEAG','LGEAR'],    'Brazil': ['LGEBR','LGESP'],    'Chile': ['LGECL'],    'Colombia': ['LGEVZ', 'LGECB'],\n",
    "    'Panama': ['Guatemala', 'LGEPS'],    'Peru': ['LGEPR']}\n",
    "continent_nation={\n",
    "    'Europe':['EUs','Austria', 'Czech Republic' ,'France' ,'Germany', 'Greece' ,'Hungary', 'Italy', 'Netherlands' ,'Poland' ,'Portugal' ,'Romania', 'Spain' ,'Sweden','United Kingdom'], \n",
    "    'Russia and CIS':['Kazakhstan','Russia', 'Ukraine', 'Latvia'],     'Africa and MiddleEast': ['Israel','Iran','Algeria', 'Egypt', 'Jordan', 'Kenya', 'Morocco','Saudi Arabia','The Republic of South Africa','Tunisia', 'U.A.E', 'Nigeria', 'Turkey'], \n",
    "    'Asia':['Korea','Australia','China','India','Indonesia','Japan','Malaysia','Philippines','Singapore','Taiwan','Thailand','Vietnam'], \n",
    "    'NorthAmerica' : ['Canada','Mexico','United States'],    'SouthAmerica' :['Argentina','Brazil','Chile','Colombia','Panama','Peru']\n",
    "    \n",
    "}\n",
    "hemisphere = {\n",
    "    'Northern': ['EUs', 'Austria', 'Czech Republic', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Spain', 'Sweden', 'United Kingdom', 'Kazakhstan', 'Russia', 'Ukraine', 'Latvia', 'Israel', 'Iran', 'Jordan', 'Morocco', 'Saudi Arabia', 'Tunisia', 'Turkey', 'Korea', 'China', 'Japan', 'Taiwan', 'Canada', 'United States', 'Mexico', 'Panama'],\n",
    "    'Southern': ['Algeria', 'Egypt', 'Kenya', 'The Republic of South Africa', 'U.A.E', 'Nigeria', 'Australia', 'India', 'Indonesia', 'Malaysia', 'Philippines', 'Singapore', 'Thailand', 'Vietnam', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Peru']\n",
    "}\n",
    "mapping_dict = {\n",
    "    \"others\": \"Other\",\n",
    "    \"Others\": \"Other\",\n",
    "    \"other_\": \"Other\",\n",
    "    \"other\": \"Other\",\n",
    "    \"Etc.\": \"ETC.\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성 및 전처리 함수 \n",
    "def get_datas():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)\n",
    "    train['is_converted']=np.where(train['is_converted']==True,1,0)\n",
    "    return train, test \n",
    "\n",
    "\n",
    "def delete_cols(data, cols):\n",
    "    data = data.drop(columns=cols)\n",
    "    return data\n",
    "\n",
    "def log_transform(data,cols):\n",
    "    for col in cols :\n",
    "        data[col+'log']=np.log1p(data[col]) \n",
    "    return data \n",
    "\n",
    "\n",
    "def eda_expected_timeline(df):\n",
    "    \n",
    "    def timeline_label(time):\n",
    "    \n",
    "        time = str(time).lower().replace(' ','').replace('_','').replace('/','').replace(',','').replace('~','').replace('&','').replace('-','').replace('.','')\n",
    "        \n",
    "        if time == 'lessthan3months':\n",
    "            result = 'less than 3 months'\n",
    "        elif time == '3months6months':\n",
    "            result = '3 months ~ 6 months'\n",
    "        elif time == '6months9months':\n",
    "            result = '6 months ~ 9 months'\n",
    "        elif time == '9months1year':\n",
    "            result = '9 months ~ 1 year'\n",
    "        elif time == 'morethanayear':\n",
    "            result = 'more than a year'\n",
    "        else:\n",
    "            result = 'aimers_0203'\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    df['expected_timeline'] = df['expected_timeline'].apply(timeline_label)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def inquiry_type_preprocessing(train, test, categorical_list):\n",
    "    df = pd.concat([train, test], axis=0)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    idx = len(train)\n",
    "    for feature in categorical_list:\n",
    "        df[feature] = df[feature].str.lower()\n",
    "        # replace() 함수를 사용하여 특수 문자 대체\n",
    "        df[feature] = df[feature].str.replace(pat=r'[^\\w\\s]', repl=r' ', regex=True)\n",
    "        df[feature] = df[feature].str.replace(pat=r'_', repl=r' ', regex=True)\n",
    "\n",
    "    token_list = df['inquiry_type'].str.split()\n",
    "    for i in range(len(df)):\n",
    "        # nan 처리\n",
    "        if type(token_list[i]) == float:\n",
    "            continue\n",
    "        if 'purchase' in token_list[i]  or 'quotation' in token_list[i]:\n",
    "            df['inquiry_type'][i] = 'purchase or quotation' \n",
    "\n",
    "        if 'partnership' in token_list[i] or 'distributorship' in token_list[i] :\n",
    "            df['inquiry_type'][i] = 'partnership' \n",
    "        \n",
    "        if 'technical' in token_list[i] :\n",
    "            df['inquiry_type'][i] = 'technical'\n",
    "\n",
    "        if 'sales' in token_list[i] :\n",
    "            df['inquiry_type'][i] = 'sales' \n",
    "            \n",
    "    df.loc[df['inquiry_type'] == 'others', 'inquiry_type'] = 'other'\n",
    "    df.loc[df['inquiry_type'] == 'other ', 'inquiry_type'] = 'other'\n",
    "    df.loc[df['inquiry_type'] == 'etc ', 'inquiry_type'] = 'other'\n",
    "\n",
    "    value_counts = df['inquiry_type'].value_counts()\n",
    "    return df[:idx], df[idx:]\n",
    "\n",
    "\n",
    "def customer_type_preprocessing(train, test):\n",
    "    df = pd.concat([train, test], axis=0)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    idx = len(train)\n",
    "    \n",
    "    df[\"customer_type\"] = df[\"customer_type\"].str.lower()\n",
    "    # replace() 함수를 사용하여 특수 문자 대체\n",
    "    df[\"customer_type\"] = df[\"customer_type\"].str.replace(pat=r'[^\\w\\s]', repl=r' ', regex=True)\n",
    "    df[\"customer_type\"] = df[\"customer_type\"].str.replace(pat=r'_', repl=r' ', regex=True)\n",
    "\n",
    "    token_list = df[\"customer_type\"].str.split()\n",
    "    for i in range(len(df)):\n",
    "        # nan 처리\n",
    "        if type(token_list[i]) == float:\n",
    "            continue\n",
    "        if 'partner' in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'partner' \n",
    "\n",
    "        if 'influencer' in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'influencer' \n",
    "        \n",
    "        if 'engineer' in token_list[i] or 'software' in token_list[i] or 'developer' in token_list[i] or \"technician\" in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'engineer' \n",
    "\n",
    "        if 'end' in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'end customer' \n",
    "        \n",
    "        if 'homeowner' in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'home owner'\n",
    "\n",
    "        if 'consultant' in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'consultant' \n",
    "\n",
    "        if 'others' in token_list[i] or 'etc' in token_list[i] or 'other ' in token_list[i]:\n",
    "            df[\"customer_type\"][i] = 'other' \n",
    "\n",
    "    return df[:idx], df[idx:]\n",
    "\n",
    "\n",
    "# total_area 변수로 통일\n",
    "def eda_business_area(df):\n",
    "    for col in ['business_area','business_subarea']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].str.replace(\" \", \"\") \n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', \"\") \n",
    "        df[col] = df[col].fillna('nan') \n",
    "    df['total_area'] = df['business_area'].astype(str) + df['business_subarea'].astype(str)\n",
    "    return df \n",
    "\n",
    "# 새로운 국가명, 대륙 열을 만들기 \n",
    "def get_nation_continent(df):\n",
    "    nation_corp_reverse ={v:k for k , values in nation_corp.items() for v in values }\n",
    "    df['nation']=df['response_corporate'].map(nation_corp_reverse)\n",
    "    continent_nation_reverse ={v:k for k , values in continent_nation.items() for v in values }\n",
    "    df['continent']=df['nation'].map(continent_nation_reverse)\n",
    "#     df = df.drop('customer_country',axis=1) \n",
    "    return df \n",
    "\n",
    "#라벨 인코딩 \n",
    "def label_encoding(series: pd.Series) -> pd.Series:\n",
    "    my_dict = {}\n",
    "    series = series.astype(str)\n",
    "    for idx, value in enumerate(sorted(series.unique())):\n",
    "        my_dict[value] = idx\n",
    "    series = series.map(my_dict)\n",
    "    return series\n",
    "\n",
    "# com_reg_ver_win_rate 최빈값으로 채우기 \n",
    "def com_reg_fill(train,test):\n",
    "    train['com_reg_ver_win_rate'] = train['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    test['com_reg_ver_win_rate'] = test['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    return train,test\n",
    "\n",
    "def timeline_tonumber(row):\n",
    "    if row['expected_timeline'] == 'less than 3 months':\n",
    "        return int(1)\n",
    "    elif row['expected_timeline'] == '3 months ~ 6 months':\n",
    "        return int(3)\n",
    "    elif row['expected_timeline'] == '6 months ~ 9 months':\n",
    "        return int(6)\n",
    "    elif row['expected_timeline'] == '9 months ~ 1 year':\n",
    "        return int(9)    \n",
    "    elif row['expected_timeline'] =='aimer_0203':\n",
    "        return np.nan\n",
    "    else : \n",
    "        return int(12)\n",
    "    \n",
    "def create_grouped_features(train, test, group, numeric_var):\n",
    "    # 범주형 특성들에 대해서 다른 수치형 데이터의 중앙값, 최대, 합을 새로운 열로 추가하기 \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    aggs = ['median', 'max','sum']\n",
    "    for agg in aggs:\n",
    "        # groupby 후 aggregation\n",
    "        a1 = train.groupby([group])[numeric_var].agg(agg).to_dict()\n",
    "        # 새로운 feature 생성\n",
    "        train[numeric_var+'_'+group+'_'+agg] = train[group].map(a1)\n",
    "        test[numeric_var+'_'+group+'_'+agg] = test[group].map(a1)\n",
    "    return train, test\n",
    "\n",
    "def do_scale(train,test, scale_cols) :\n",
    "    for c in scale_cols:\n",
    "        min_value = train[c].min()\n",
    "        max_value = train[c].max()\n",
    "        train[c] = (train[c] - min_value) / (max_value - min_value)\n",
    "        test[c] = (test[c] - min_value) / (max_value - min_value)\n",
    "    return train,test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['business_unit','customer_idx']\n",
    "numeric_vars = ['historical_existing_cnt', 'lead_desc_length']\n",
    "scale_cols = ['com_reg_ver_win_rate','historical_existing_cnt', 'lead_desc_length','ver_win_rate_x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 갖고오기 \n",
    "train,test= get_datas() \n",
    "\n",
    "# 스케일링 하기 \n",
    "train,test =do_scale(train,test,scale_cols)\n",
    "# 범주형 데이터에 대해 수치형 데이터 통계값 추가\n",
    "for group in groups:\n",
    "    for numeric_var in numeric_vars:\n",
    "        train, test = create_grouped_features(train, test, group, numeric_var)\n",
    "        \n",
    "# 전처리, 로그변환 수행하기 \n",
    "columns_to_log=['com_reg_ver_win_rate','lead_desc_length']\n",
    "train,test= log_transform(train,columns_to_log ),log_transform(test,columns_to_log)\n",
    "train,test =eda_business_area(train),eda_business_area(test)\n",
    "train,test= get_nation_continent(train),get_nation_continent(test)\n",
    "train,test=eda_expected_timeline(train) ,eda_expected_timeline(test)\n",
    "train,test=customer_type_preprocessing(train, test)\n",
    "train,test=inquiry_type_preprocessing(train, test, ['inquiry_type']) \n",
    "\n",
    "for col in ['customer_idx','customer_type',]:\n",
    "    train[col+'count'] =train[col].map(train[col].value_counts())\n",
    "    test[col+'count'] =test[col].map(train[col].value_counts())\n",
    "    \n",
    "\n",
    "train['idx_unit'] = train['customer_idx'].astype(str)+train['business_unit'].astype(str)\n",
    "test['idx_unit'] = test['customer_idx'].astype(str)+test['business_unit'].astype(str)\n",
    "train['idx_posi'] = train['customer_idx'].astype(str)+train['customer_position'].astype(str)\n",
    "test['idx_posi'] = test['customer_idx'].astype(str)+test['customer_position'].astype(str)\n",
    "train['conti_inquiry'] = train['continent'].astype(str)+train['inquiry_type'].astype(str)\n",
    "test['conti_inquiry'] = test['continent'].astype(str)+test['inquiry_type'].astype(str)\n",
    "# train['job_unit'] = train['business_unit'].astype(str)+train['customer_job'].astype(str)\n",
    "# test['job_unit'] = test['business_unit'].astype(str)+test['customer_job'].astype(str)\n",
    "#0.717 나옴 4개 변수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country.1지우지말기 \n",
    "columns_to_delete=['customer_country']\n",
    "# columns_to_delete=[]\n",
    "train,test =delete_cols(train, columns_to_delete), delete_cols(test,columns_to_delete)\n",
    "\n",
    "cols = [     'customer_country',    \"business_subarea\",    \"business_area\",    \"business_unit\",    \"customer_type\",    \"enterprise\",    \"customer_job\",    \"inquiry_type\",    \"product_category\",    \"product_subcategory\",    \"product_modelname\",    \"customer_position\",\n",
    "      'customer_country.1', \"response_corporate\", \"expected_timeline\",\n",
    "'nation','continent','lead_owner','idx_posi', 'conti_inquiry', 'idx_unit','bant_submit'\n",
    ",'total_area'   ]\n",
    "label_columns =list(set(cols)-set(columns_to_delete))\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "enc = CatBoostEncoder(cols=label_columns)\n",
    "enc.fit(train[label_columns], train['is_converted'])  # 'target'은 실제 데이터의 타겟 변수 이름에 맞게 변경\n",
    "# 인코딩 적용\n",
    "train[label_columns] = enc.transform(train[label_columns])\n",
    "test[label_columns] = enc.transform(test[label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)\n",
    "train = train.fillna(0)\n",
    "x = train.drop('is_converted', axis=1)\n",
    "y = train.is_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('is_converted', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.89961, 정밀도: 0.9322399999999998, 재현율: 0.86928, ROC-AUC: 0.9959477578725165\n",
      "> 평균 검증 오차행렬: \n",
      " [[5414.2   30.7]\n",
      " [  63.4  421.6]]\n",
      "> F1 Score: 0.89961, 정밀도: 0.9322399999999998, 재현율: 0.86928, ROC-AUC: 0.9959477578725165\n",
      "> 평균 검증 오차행렬: \n",
      " [[5414.2   30.7]\n",
      " [  63.4  421.6]]\n",
      "> F1 Score: 0.89961, 정밀도: 0.9322399999999998, 재현율: 0.86928, ROC-AUC: 0.9959477578725165\n",
      "> 평균 검증 오차행렬: \n",
      " [[5414.2   30.7]\n",
      " [  63.4  421.6]]\n",
      "> F1 Score: 0.89961, 정밀도: 0.9322399999999998, 재현율: 0.86928, ROC-AUC: 0.9959477578725165\n",
      "> 평균 검증 오차행렬: \n",
      " [[5414.2   30.7]\n",
      " [  63.4  421.6]]\n",
      "> F1 Score: 0.89961, 정밀도: 0.9322399999999998, 재현율: 0.86928, ROC-AUC: 0.9959477578725165\n",
      "> 평균 검증 오차행렬: \n",
      " [[5414.2   30.7]\n",
      " [  63.4  421.6]]\n",
      "0.89961 0.9959477578725165 421.6 5414.2\n"
     ]
    }
   ],
   "source": [
    "def LGBM_skfold(zero_wei,one_wei,seed) :\n",
    "     #Decisiontree에 대해서만 skfold 적용하는 함수 \n",
    "    real_preds = []\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    model = LGBMClassifier(random_state=seed ,class_weight =class_weight, verbose=-1)\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "#     Skfold = StratifiedShuffleSplit(n_splits=10, random_state=42)  \n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    tt = []\n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test)\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴\n",
    "\n",
    "f1_avg,roc_avg,tt=0,0,0\n",
    "avg_get_1 =0\n",
    "for seed in [5,11,30,322,8940]:\n",
    "    _,f1,roc,ones,tts =LGBM_skfold(1,1,seed)\n",
    "    f1_avg+= f1 \n",
    "    roc_avg+= roc \n",
    "    avg_get_1+=ones\n",
    "    tt+= tts \n",
    "print(f1_avg/5,roc_avg/5,avg_get_1/5, tt/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.89961, 정밀도: 0.9322399999999998, 재현율: 0.86928, ROC-AUC: 0.9959477578725165\n",
      "> 평균 검증 오차행렬: \n",
      " [[5414.2   30.7]\n",
      " [  63.4  421.6]]\n",
      "Count of 0: 4584\n",
      "Count of 1: 687\n"
     ]
    }
   ],
   "source": [
    "LGBM_preds,_,_,_,_= LGBM_skfold(1,1,3)\n",
    "\n",
    "LGBM_predicts_array = np.array(LGBM_preds)\n",
    "LGBM_final_prediction = np.mean(LGBM_predicts_array, axis=0)\n",
    "LGBM_final_prediction = np.where(LGBM_final_prediction < 0.0000000001, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(LGBM_final_prediction == 0))\n",
    "count_2 = np.size(np.where(LGBM_final_prediction >0))\n",
    "count_1 = np.size(np.where(LGBM_final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.89606, 정밀도: 0.9287000000000001, 재현율: 0.86578, ROC-AUC: 0.9958321375367852\n",
      "> 평균 검증 오차행렬: \n",
      " [[5412.6   32.3]\n",
      " [  65.1  419.9]]\n",
      "> F1 Score: 0.89606, 정밀도: 0.9287000000000001, 재현율: 0.86578, ROC-AUC: 0.9958321375367852\n",
      "> 평균 검증 오차행렬: \n",
      " [[5412.6   32.3]\n",
      " [  65.1  419.9]]\n",
      "> F1 Score: 0.89606, 정밀도: 0.9287000000000001, 재현율: 0.86578, ROC-AUC: 0.9958321375367852\n",
      "> 평균 검증 오차행렬: \n",
      " [[5412.6   32.3]\n",
      " [  65.1  419.9]]\n",
      "> F1 Score: 0.89606, 정밀도: 0.9287000000000001, 재현율: 0.86578, ROC-AUC: 0.9958321375367852\n",
      "> 평균 검증 오차행렬: \n",
      " [[5412.6   32.3]\n",
      " [  65.1  419.9]]\n",
      "> F1 Score: 0.89606, 정밀도: 0.9287000000000001, 재현율: 0.86578, ROC-AUC: 0.9958321375367852\n",
      "> 평균 검증 오차행렬: \n",
      " [[5412.6   32.3]\n",
      " [  65.1  419.9]]\n",
      "0.89606 0.9958321375367852 419.9 5412.6\n"
     ]
    }
   ],
   "source": [
    "def XGB_skfold(zero_wei,one_wei,seed) :\n",
    "     #Decisiontree에 대해서만 skfold 적용하는 함수 \n",
    "    real_preds = []\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    model = XGBClassifier(random_state=seed ,class_weight =class_weight, verbose=-1)\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "#     Skfold = StratifiedShuffleSplit(n_splits=10, random_state=42)  \n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    tt = []\n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test)\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴\n",
    "\n",
    "f1_avg,roc_avg,tt=0,0,0\n",
    "avg_get_1 =0\n",
    "for seed in [5,11,30,322,8940]:\n",
    "    _,f1,roc,ones,tts =XGB_skfold(1,1,seed)\n",
    "    f1_avg+= f1 \n",
    "    roc_avg+= roc \n",
    "    avg_get_1+=ones\n",
    "    tt+= tts \n",
    "print(f1_avg/5,roc_avg/5,avg_get_1/5, tt/5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.89606, 정밀도: 0.9287000000000001, 재현율: 0.86578, ROC-AUC: 0.9958321375367852\n",
      "> 평균 검증 오차행렬: \n",
      " [[5412.6   32.3]\n",
      " [  65.1  419.9]]\n",
      "Count of 0: 4496\n",
      "Count of 1: 775\n"
     ]
    }
   ],
   "source": [
    "XGB_preds,_,_,_,_= XGB_skfold(1,1,100)\n",
    "\n",
    "XGB_predicts_array = np.array(XGB_preds)\n",
    "XGB_final_prediction = np.mean(XGB_predicts_array, axis=0)\n",
    "XGB_final_prediction = np.where(XGB_final_prediction < 0.0000000001, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(XGB_final_prediction == 0))\n",
    "count_2 = np.size(np.where(XGB_final_prediction >0))\n",
    "count_1 = np.size(np.where(XGB_final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.8612299999999999, 정밀도: 0.8573999999999999, 재현율: 0.8651500000000001, ROC-AUC: 0.9275408477911682\n",
      "> 평균 검증 오차행렬: \n",
      " [[5375.    69.9]\n",
      " [  65.4  419.6]]\n",
      "> F1 Score: 0.8632500000000001, 정밀도: 0.8630799999999998, 재현율: 0.86351, ROC-AUC: 0.9272356743221561\n",
      "> 평균 검증 오차행렬: \n",
      " [[5378.4   66.5]\n",
      " [  66.2  418.8]]\n",
      "> F1 Score: 0.8630599999999999, 정밀도: 0.86047, 재현율: 0.86577, ROC-AUC: 0.9278020625220285\n",
      "> 평균 검증 오차행렬: \n",
      " [[5376.7   68.2]\n",
      " [  65.1  419.9]]\n",
      "> F1 Score: 0.8627, 정밀도: 0.8598800000000001, 재현율: 0.86557, ROC-AUC: 0.9277747776911024\n",
      "> 평균 검증 오차행렬: \n",
      " [[5376.4   68.5]\n",
      " [  65.2  419.8]]\n",
      "> F1 Score: 0.8609499999999999, 정밀도: 0.8575200000000001, 재현율: 0.86453, ROC-AUC: 0.9272422804050343\n",
      "> 평균 검증 오차행렬: \n",
      " [[5375.1   69.8]\n",
      " [  65.7  419.3]]\n",
      "0.862238 0.9275191285462979 419.48 5376.32\n"
     ]
    }
   ],
   "source": [
    "def dtc_skfold(zero_wei,one_wei,seed) :\n",
    "     #Decisiontree에 대해서만 skfold 적용하는 함수 \n",
    "    real_preds = []\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    model = DecisionTreeClassifier(random_state=seed ,class_weight =class_weight)\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "#     Skfold = StratifiedShuffleSplit(n_splits=10, random_state=42)  \n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    tt = []\n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test)\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴\n",
    "\n",
    "f1_avg,roc_avg,tt=0,0,0\n",
    "avg_get_1 =0\n",
    "for seed in [5,11,30,322,8940]:\n",
    "    _,f1,roc,ones,tts =dtc_skfold(1,1,seed)\n",
    "    f1_avg+= f1 \n",
    "    roc_avg+= roc \n",
    "    avg_get_1+=ones\n",
    "    tt+= tts \n",
    "print(f1_avg/5,roc_avg/5,avg_get_1/5, tt/5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.86283, 정밀도: 0.8610800000000001, 재현율: 0.8647500000000001, ROC-AUC: 0.9276289726627786\n",
      "> 평균 검증 오차행렬: \n",
      " [[5377.1   67.8]\n",
      " [  65.6  419.4]]\n",
      "Count of 0: 3056\n",
      "Count of 1: 2215\n"
     ]
    }
   ],
   "source": [
    "dtc_preds,_,_,_,_= dtc_skfold(1,1,3)\n",
    "\n",
    "dtc_predicts_array = np.array(dtc_preds)\n",
    "dtc_final_prediction = np.mean(dtc_predicts_array, axis=0)\n",
    "dtc_final_prediction = np.where(dtc_final_prediction < 0.1, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(dtc_final_prediction == 0))\n",
    "count_2 = np.size(np.where(dtc_final_prediction >0))\n",
    "count_1 = np.size(np.where(dtc_final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(LGBM_final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGBM_final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "775"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(XGB_final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_true = 0\n",
    "for i in range(len(XGB_final_prediction)):\n",
    "    if XGB_final_prediction[i] == 1 and dtc_final_prediction[i] == 1:\n",
    "        con_true += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_true = 0\n",
    "for i in range(len(XGB_final_prediction)):\n",
    "    if LGBM_final_prediction[i] == 1 and dtc_final_prediction[i] == 1:\n",
    "        con_true += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2240\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(dtc_final_prediction)):\n",
    "    if dtc_final_prediction[idx] == 0:\n",
    "        change = 0\n",
    "        if LGBM_final_prediction[idx] == 1 or XGB_final_prediction[idx] == 1:\n",
    "            change = 1\n",
    "        dtc_final_prediction[idx] = change\n",
    "\n",
    "print(sum(dtc_final_prediction))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed(3)에 대해서만 제출 할 때 \n",
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= dtc_final_prediction\n",
    "sub.to_csv('XGB_LGBM_DTC.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
