{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import re \n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import category_encoders as ce \n",
    "\n",
    "from sklearn.metrics import (    accuracy_score,    confusion_matrix,f1_score,precision_score, recall_score,roc_auc_score)\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold,cross_val_score, StratifiedShuffleSplit\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE , ADASYN\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,HistGradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier,StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_corp = {\n",
    "    'Austria': ['LGEAG'],    'Czech Republic': ['LGECZ'],    'France': ['LGEFS'],    'Germany': ['LGEDG'],    'Greece': ['LGEHS'],    'Hungary': ['LGEMK'],    'Italy': ['LGEIS'],    'Netherlands': ['LGESC', 'LGEEH', 'LGEBN'],    'Poland': ['LGEWR', 'LGEPL', 'LGEMA'],    'Portugal': ['LGEPT','LGEBT'],\n",
    "    'EUs': ['LGEEB'],    'Romania': ['LGERO'],    'Spain': ['LGEES'],    'Sweden': ['LGENO', 'LGESW'],    'United Kingdom': ['LGEUK'],      'Kazakhstan': ['LGEAK'],    'Russia': ['LGERM', 'LGERI', 'LGERA'],\n",
    "    'Ukraine': ['LGEUR'],    'Latvia': ['LGELV','LGELA'],    'Algeria': ['LGEAS'],\n",
    "    'Egypt': ['LGEEG'],    'Jordan': ['LGELF'],    'Kenya': ['LGESK','LGEEF'],    'Morocco': ['LGEMC'],\n",
    "    'Saudi Arabia': ['LGESJ'],    'Iran':['LGEIR'],     'Israel':['LGEYK'],     'The Republic of South Africa': ['LGESA'],\n",
    "    'Tunisia': ['LGETU'],    'U.A.E': ['LGEOT', 'LGEDF', 'LGEGF', 'LGEME', 'LGEAF'],    'Nigeria': ['LGEAO', 'LGENI'],\n",
    "    'Turkey': ['LGETK', 'LGEAT'],    'Australia': ['LGEAP'],\n",
    "    'China': ['LGEQA', 'LGETL', 'LGECH', 'LGEYT', 'LGETR', 'LGETA', 'LGESY', 'LGESH', 'LGEQH', 'LGEQD', 'LGEPN', 'LGEND', 'LGEKS', 'LGEHZ', 'LGEHN', 'LGEHK'],\n",
    "    'India': ['LGEIL'],    'Indonesia': ['LGEIN'],    'Japan': ['LGEJP'],    'Malaysia': ['LGEML'],    'Philippines': ['LGEPH'],\n",
    "    'Singapore': ['LGESL'],    'Taiwan': ['LGETT'],    'Korea' :['LGEKR'],    'Thailand': ['LGETH'],    'Vietnam': ['LGEVN','LGEVH'],\n",
    "     'Canada': ['LGECI'],    'Mexico': ['LGERS', 'LGEMX', 'LGEMS', 'LGEMM'],    'United States': ['LGEMR', 'LGEUS', 'LGEMU', 'LGEAI'],\n",
    "    'Argentina': ['LGEAG','LGEAR'],    'Brazil': ['LGEBR','LGESP'],    'Chile': ['LGECL'],    'Colombia': ['LGEVZ', 'LGECB'],\n",
    "    'Panama': ['Guatemala', 'LGEPS'],    'Peru': ['LGEPR']}\n",
    "continent_nation={\n",
    "    'Europe':['EUs','Austria', 'Czech Republic' ,'France' ,'Germany', 'Greece' ,'Hungary', 'Italy', 'Netherlands' ,'Poland' ,'Portugal' ,'Romania', 'Spain' ,'Sweden','United Kingdom'], \n",
    "    'Russia and CIS':['Kazakhstan','Russia', 'Ukraine', 'Latvia'],     'Africa and MiddleEast': ['Israel','Iran','Algeria', 'Egypt', 'Jordan', 'Kenya', 'Morocco','Saudi Arabia','The Republic of South Africa','Tunisia', 'U.A.E', 'Nigeria', 'Turkey'], \n",
    "    'Asia':['Korea','Australia','China','India','Indonesia','Japan','Malaysia','Philippines','Singapore','Taiwan','Thailand','Vietnam'], \n",
    "    'NorthAmerica' : ['Canada','Mexico','United States'],    'SouthAmerica' :['Argentina','Brazil','Chile','Colombia','Panama','Peru']\n",
    "    \n",
    "}\n",
    "hemisphere = {\n",
    "    'Northern': ['EUs', 'Austria', 'Czech Republic', 'France', 'Germany', 'Greece', 'Hungary', 'Italy', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Spain', 'Sweden', 'United Kingdom', 'Kazakhstan', 'Russia', 'Ukraine', 'Latvia', 'Israel', 'Iran', 'Jordan', 'Morocco', 'Saudi Arabia', 'Tunisia', 'Turkey', 'Korea', 'China', 'Japan', 'Taiwan', 'Canada', 'United States', 'Mexico', 'Panama'],\n",
    "    'Southern': ['Algeria', 'Egypt', 'Kenya', 'The Republic of South Africa', 'U.A.E', 'Nigeria', 'Australia', 'India', 'Indonesia', 'Malaysia', 'Philippines', 'Singapore', 'Thailand', 'Vietnam', 'Argentina', 'Brazil', 'Chile', 'Colombia', 'Peru']\n",
    "}\n",
    "mapping_dict = {\n",
    "#     \"Toi muon tim hieu thong tin ky thuat, gia ca cua sp de su dung\": \"Product Information\",\n",
    "#     \"tôi cần tham khảo giá và giải pháp từ LG\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Vui lòng báo giá giúp mình sản phẩm đo thân nhiệt Xin cảm ơn\": \"Request for quotation or purchase\",\n",
    "#     \"LED Signage\": \"Product Information\",\n",
    "#     \"Standalone\": \"Product Information\",\n",
    "#     \"for school\": \"Other\",\n",
    "#     \"Not specified\": \"Other\",\n",
    "#     \"Intégrateur historique du George V\": \"Other\",\n",
    "#     \"Solicito apoyo para realizar cotizacion de los dispositivos que ofrecen en la solución One Quick:\": \"Quotation or Purchase Consultation\",\n",
    "#     \"Pantallas Interactivas para Clinicas\": \"Product Information\",\n",
    "#     \"Hotel TV products\": \"Product Information\",\n",
    "#     \"VRF\": \"Product Information\",\n",
    "#     \"Preciso de um monitor médico para radiografia convencional e tomogrtafia.\": \"Sales Inquiry\",\n",
    "    \"others\": \"Other\",\n",
    "    \"Others\": \"Other\",\n",
    "    \"other_\": \"Other\",\n",
    "    \"other\": \"Other\",\n",
    "    \"Etc.\": \"ETC.\",\n",
    "#     \"window facing product\": \"Product Information\",\n",
    "#     \"Digital platform\": \"Product Information\",\n",
    "#     \"(Select ID_Needs)\": \"Other\",\n",
    "#     \"One Quick:Flex\": \"Product Information\",\n",
    "#     \"AIO\": \"Product Information\",\n",
    "#     \"Needs\": \"Other\",\n",
    "#     \"Hospital TV\": \"Product Information\",\n",
    "#     \"i want to know the details about it\": \"Product Information\",\n",
    "#     \"EDUCATIONAL EQUIPMENTS\": \"Product Information\",\n",
    "#     \"TV interactive\": \"Product Information\",\n",
    "#     \"Hola me pueden cotizar 19 pantallas interactivas de 100 pulgadas entregadas en Guayaquil -Ecuador.\": \"Request for quotation or purchase\",\n",
    "#     \"teach\": \"Other\",\n",
    "#     \"Display Textbook and photos\": \"Usage or technical consultation\",\n",
    "#     \"High inch 86 / 98 or 110\": \"Product Information\",\n",
    "#     \"quotation_\": \"Request for quotation or purchase\",\n",
    "#     \"display product\": \"Product Information\",\n",
    "#     \"first Info and pricing\": \"Quotation or Purchase Consultation\",\n",
    "#     \"estoy buscando para Ecuador este producto LG MAGNIT micro LED, para un cliente de 138 pulgadas, con envió marítimo.\": \"Sales Inquiry\",\n",
    "#     \"Evento_SdelEstero\": \"Other\",\n",
    "#     \"probeam precio\": \"Sales Inquiry\",\n",
    "#     \"media inquiry\": \"Sales Inquiry\",\n",
    "#     \"Video Wall\": \"Product Information\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 생성 및 전처리 함수 \n",
    "def get_datas():\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"submission.csv\").drop(['id','is_converted'], axis =1) # 테스트 데이터(제출파일의 데이터)\n",
    "    train['is_converted']=np.where(train['is_converted']==True,1,0)\n",
    "    return train, test \n",
    "\n",
    "\n",
    "def delete_cols(data, cols):\n",
    "    data = data.drop(columns=cols)\n",
    "    return data\n",
    "\n",
    "def log_transform(data,cols):\n",
    "    for col in cols :\n",
    "        data[col+'log']=np.log1p(data[col]) \n",
    "    return data \n",
    "\n",
    "\n",
    "def eda_expected_timeline(df):\n",
    "    \n",
    "    def timeline_label(time):\n",
    "    \n",
    "        time = str(time).lower().replace(' ','').replace('_','').replace('/','').replace(',','').replace('~','').replace('&','').replace('-','').replace('.','')\n",
    "        \n",
    "        if time == 'lessthan3months':\n",
    "            result = 'less than 3 months'\n",
    "        elif time == '3months6months':\n",
    "            result = '3 months ~ 6 months'\n",
    "        elif time == '6months9months':\n",
    "            result = '6 months ~ 9 months'\n",
    "        elif time == '9months1year':\n",
    "            result = '9 months ~ 1 year'\n",
    "        elif time == 'morethanayear':\n",
    "            result = 'more than a year'\n",
    "        else:\n",
    "            result = 'aimers_0203'\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    df['expected_timeline'] = df['expected_timeline'].apply(timeline_label)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# inquiry type 전처리하기 \n",
    "def eda_inquiry_type(df):\n",
    "    df['inquiry_type']= df['inquiry_type'].map(mapping_dict).fillna(train['inquiry_type'])\n",
    "    df.loc[df['inquiry_type'].str.contains('Solicito apoyo para realizar', na=False), 'inquiry_type'] = 'Quotation or Purchase Consultation'\n",
    "    df['inquiry_type'] = df['inquiry_type'].str.lower()\n",
    "    replacement = {'/': ' ', '-':' ', '_':' '}\n",
    "    df['inquiry_type'].replace(replacement, regex=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#customer type 처리 \n",
    "def customer_type(data):\n",
    "    data['customer_type']=data['customer_type'].fillna('none') \n",
    "    return data\n",
    "\n",
    "# total_area 변수로 통일\n",
    "def eda_business_area(df):\n",
    "    for col in ['business_area','business_subarea']:\n",
    "        df[col] = df[col].str.lower()\n",
    "        df[col] = df[col].str.replace(\" \", \"\") \n",
    "        df[col] = df[col].str.replace(r'[^\\w\\s]', \"\") \n",
    "        df[col] = df[col].fillna('nan') \n",
    "    df['total_area'] = df['business_area'].astype(str) + df['business_subarea'].astype(str)\n",
    "    return df \n",
    "\n",
    "# 새로운 국가명, 대륙 열을 만들기 \n",
    "def get_nation_continent(df):\n",
    "    nation_corp_reverse ={v:k for k , values in nation_corp.items() for v in values }\n",
    "    df['nation']=df['response_corporate'].map(nation_corp_reverse)\n",
    "    continent_nation_reverse ={v:k for k , values in continent_nation.items() for v in values }\n",
    "    df['continent']=df['nation'].map(continent_nation_reverse)\n",
    "#     df = df.drop('customer_country',axis=1) \n",
    "    return df \n",
    "\n",
    "#라벨 인코딩 \n",
    "def label_encoding(series: pd.Series) -> pd.Series:\n",
    "    my_dict = {}\n",
    "    series = series.astype(str)\n",
    "    for idx, value in enumerate(sorted(series.unique())):\n",
    "        my_dict[value] = idx\n",
    "    series = series.map(my_dict)\n",
    "    return series\n",
    "\n",
    "# com_reg_ver_win_rate 최빈값으로 채우기 \n",
    "def com_reg_fill(train,test):\n",
    "    train['com_reg_ver_win_rate'] = train['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    test['com_reg_ver_win_rate'] = test['com_reg_ver_win_rate'].fillna(train['com_reg_ver_win_rate'].mode()[0])\n",
    "    return train,test\n",
    "\n",
    "#****************************Feature Engineering*************************************#\n",
    "\n",
    "# area,unit,continent ->comregverwin , fe9 \n",
    "# area,unit ->ver_win_ratio_per_bu\n",
    "# unit, continent -> fe1 \n",
    "# owner ,unit  -> fe3 \n",
    "# nation, job -> fe10 \n",
    "\n",
    "# 회사별 Quotation or Purchase Consultation,Request for Partnership의 횟수\n",
    "# PortugalChina U.A.E  United States Argentina\n",
    "# 회사별 관료직, 혹은 성공률 높은 이들의 빈도 \n",
    "# unit, position 로 그룹화   x \n",
    "# 국가별 지점의 개수  \n",
    "# 국가별 성공률이 0.1이 넘는 국가 1 나머지 0 \n",
    "\n",
    "\n",
    "def fe_1(train,test):\n",
    "    # unit continent으로 엮어서 영업 전환율 살펴보기 -> 'unit_conti_mean'열 새로 생성\n",
    "    # 대륙별로 어느 사업부에 영업 성공율이 높은 지 \n",
    "    se=train.groupby(['business_unit','continent'])['is_converted'].agg(['mean'])\n",
    "    se = se.rename(columns={'mean':'unit_conti_mean'})\n",
    "    train =train.merge(se, on=['business_unit','continent'], how ='left')\n",
    "    test =test.merge(se, on=['business_unit','continent'], how ='left')\n",
    "    return train,test \n",
    "\n",
    "def fe_2(train,test):\n",
    "    # 영업 당담자가 어느 정도로 다양한 회사(customer_idx)을 담당하고 있는 지 \n",
    "    \n",
    "#     count = train.groupby('lead_owner').size().reset_index(name='leadowner_cnt')     \n",
    "#     train = train.merge(count, on='lead_owner', how='left')\n",
    "#     train['leadowner_cnt']= np.log1p(train['leadowner_cnt'])\n",
    "#     test = test.merge(count, on='lead_owner', how= 'left')\n",
    "#     test['leadowner_cnt']=np.log1p(test['leadowner_cnt'])\n",
    "    unique_count = train.groupby('lead_owner')['customer_idx'].nunique().reset_index(name='unique_cusidx_cnt')\n",
    "    train = train.merge(unique_count, on='lead_owner', how='left')\n",
    "    test = test.merge(unique_count,on ='lead_owner',how ='left')\n",
    "    train['unique_cusidx_cnt']= np.log1p(train['unique_cusidx_cnt'])\n",
    "    test['unique_cusidx_cnt']= np.log1p(test['unique_cusidx_cnt'])\n",
    "    \n",
    "    return train, test \n",
    "\n",
    "def fe_3(train,test):\n",
    "    # 영업담당자와 사업부로 영업전환 성공률 살펴보기 -> 어느 사업부를 어느 담당자가 담당해야 성공율이 높나 확인 \n",
    "\n",
    "    se = train.groupby(['lead_owner','business_unit'])['is_converted'].agg(['mean']).rename(columns={'mean': 'owner_unit_mean'})\n",
    "    train = train.merge(se, on=['lead_owner','business_unit'], how='left')\n",
    "    test = test.merge(se, on=['lead_owner','business_unit'],how='left')\n",
    "    return train, test\n",
    "\n",
    "def fe_4(train,test):\n",
    "    # customer_idx가 대기업, 중소기업으로 분류되는 경우 1을 부여 \n",
    "    se = train[train.groupby('customer_idx')['enterprise'].transform('nunique') > 1]\n",
    "    multi_company=list(se['customer_idx'].unique())\n",
    "    train['multi_company']=np.where(train['customer_idx'].isin(multi_company) ,1,0)\n",
    "    test['multi_company']=np.where(test['customer_idx'].isin(multi_company) ,1,0)\n",
    "    return train, test\n",
    "\n",
    "def fe_5(train,test):\n",
    "    # LG지점 , 사업부 , bantsubmit으로 영업 성공율 살펴보기 -> 너무 과적합됨으로 제외 \n",
    "    se = train.groupby(['response_corporate','business_unit','bant_submit'])['is_converted'].agg(['mean']).rename(columns={'mean':'idx_unit_mean'})\n",
    "    train=train.merge(se,on=['response_corporate','business_unit','bant_submit'], how ='left')\n",
    "    test=test.merge(se,on=['response_corporate','business_unit','bant_submit'], how ='left')\n",
    "    return train, test\n",
    "def fe_6(train,test):\n",
    "    # 영업사원, 사업부, bandsubmit 으로 영업 성공율 살펴보기 -> 과적합으로 제외 \n",
    "    se = train.groupby(['lead_owner','business_unit','bant_submit'])['is_converted'].agg(['mean']).rename(columns={'mean':'idx_unit_mean'})\n",
    "    train=train.merge(se,on=['lead_owner','business_unit','bant_submit'], how ='left')\n",
    "    test =test.merge(se,on =['lead_owner','business_unit','bant_submit'], how ='left')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_7(train,test):\n",
    "    # bant submit 제곱하기 -> isconverted와 corr는 더 높지만 성능향상은 없음 \n",
    "    train['bant_submit']=train['bant_submit']*train['bant_submit']\n",
    "    test['bant_submit']=test['bant_submit']*test['bant_submit']\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def fe_8(df):\n",
    "    # 국가별로 북반구와 남반구 특성을 생성하기 \n",
    "    hemisphere_reverse ={v:k for k , values in hemisphere.items() for v in values }\n",
    "    df['hemisphere'] =df['nation'].map(hemisphere_reverse)\n",
    "    return df \n",
    "\n",
    "def fe_9(train,test):\n",
    "    # 대륙별,사업 분야별, 사업부로 영업 성공률 살피기 \n",
    "    se=train.groupby(['business_area','business_unit','continent'])['is_converted'].agg(['mean'])\n",
    "    se = se.rename(columns={'mean':'area_unit_conti_mean'})\n",
    "    train =train.merge(se, on=['business_area','business_unit','continent'], how ='left')\n",
    "    test =test.merge(se, on=['business_area','business_unit','continent'], how ='left')\n",
    "    return train, test\n",
    "\n",
    "def fe_10(train,test):\n",
    "    #국가별, 고객의 직업에 따라서 영업 성공율 살펴보기 \n",
    "    se =train.groupby(['nation','customer_job'])['is_converted'].agg(['mean']).rename(columns={'mean':'nat_job_mean'})\n",
    "    \n",
    "    train =train.merge(se, on =['nation','customer_job'], how = 'left')\n",
    "    train['nat_job_mean']=train['nat_job_mean'].fillna(train['nat_job_mean'].mean())\n",
    "    \n",
    "    test =test.merge(se, on =['nation','customer_job'], how = 'left')\n",
    "    test['nat_job_mean']=test['nat_job_mean'].fillna(train['nat_job_mean'].mean())\n",
    "    return train,test \n",
    "\n",
    "\n",
    "\n",
    "# def fe_11(train,test):\n",
    "#     se1 =train.groupby(['business_unit','business_area'])['is_converted'].agg(['mean']).rename(columns={'mean':'new_perbu'})\n",
    "#     train =train.merge(se1,on=['business_unit','business_area'], how='left')\n",
    "#     test =test.merge(se1,on=['business_unit','business_area'], how='left')\n",
    "#     train = train.drop('ver_win_ratio_per_bu', axis = 1)\n",
    "    \n",
    "#     test = test.drop('ver_win_ratio_per_bu', axis = 1)\n",
    "#     return train,test \n",
    "\n",
    "def fe_12(train,test):\n",
    "    train['com_product'] = train['product_category'].apply(lambda x: 1 if 'signage' in str(x) else 0)\n",
    "    \n",
    "    se= train.groupby(['customer_idx'])['com_product'].agg(['mean']).rename(columns={'mean':'com_prod_mean'})\n",
    "    train = train.merge(se, on =['customer_idx'], how ='left')\n",
    "    test = test.merge(se, on =['customer_idx'], how ='left')\n",
    "    train= train.drop('com_product', axis= 1 )\n",
    "    return train,test\n",
    "    \n",
    "def fe_13(train,test):\n",
    "    se = train.groupby(['nation', 'inquiry_type'])['is_converted'].agg(['mean']).rename(columns={'mean':'nat_inquiry_type_mean'})\n",
    "    train =train.merge(se,on=['nation', 'inquiry_type'],how='left')\n",
    "    train['nat_inquiry_type_mean']=train['nat_inquiry_type_mean'].fillna(train['nat_inquiry_type_mean'].mean())\n",
    "    \n",
    "    test =test.merge(se,on=['nation', 'inquiry_type'],how='left')\n",
    "    test['nat_inquiry_type_mean']=test['nat_inquiry_type_mean'].fillna(train['nat_inquiry_type_mean'].mean())\n",
    "    return train,test\n",
    "\n",
    "def fe_14(train,test):\n",
    "    se =train.groupby(['nation'])['response_corporate'].agg(['count']).rename(columns={'count':'nr_count'})\n",
    "    train = train.merge(se, on='nation', how='left')\n",
    "    test  = test.merge(se,on='nation',how='left')\n",
    "    return train,test \n",
    "\n",
    "\n",
    "def fe_15(train,test):\n",
    "    se =train.groupby(['business_unit','continent','customer_position'])['is_converted'].agg(['mean']).rename(columns={'mean':'unit_conti_pos'})\n",
    "    \n",
    "    train =train.merge(se, on =['business_unit','continent','customer_position'], how = 'left')\n",
    "    train['unit_conti_pos']=train['unit_conti_pos'].fillna(train['unit_conti_pos'].mean())\n",
    "    test =test.merge(se, on =['business_unit','continent','customer_position'], how = 'left')\n",
    "    test['unit_conti_pos']=test['unit_conti_pos'].fillna(train['unit_conti_pos'].mean())\n",
    "    return train,test\n",
    "\n",
    "def fe_16(train,test):\n",
    "    se =train.groupby(['continent','total_area'])['is_converted'].agg(['mean']).rename(columns={'mean':'total_conti_pos'})\n",
    "    train =train.merge(se, on =['continent','total_area'], how = 'left')\n",
    "    train['total_conti_pos']=train['total_conti_pos'].fillna(train['total_conti_pos'].mean())\n",
    "    test =test.merge(se, on =['continent','total_area'], how = 'left')\n",
    "    test['total_conti_pos']=test['total_conti_pos'].fillna(train['total_conti_pos'].mean())\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def fe_17(train,test):\n",
    "    train['good']=0 \n",
    "    test['good'] = 0 \n",
    "    train.loc[train['nation'].isin(['Taiwan', 'Latvia','Czech Republic','China','Romania','Morocco','Portugal','Thailand','Argentina','U.A.E','United States']), 'good'] = 1\n",
    "    test.loc[test['nation'].isin(['Taiwan', 'Latvia','Czech Republic','China','Romania','Morocco','Portugal','Thailand','Argentina','U.A.E','United States']), 'good'] = 1\n",
    "    return train,test \n",
    "\n",
    "def fe_18(train,test, col1,col2):\n",
    "    time_avg = train[[col2, 'is_converted']].groupby(col2).mean()\n",
    "    time_avg.columns = [f'{col2}_avg']\n",
    "\n",
    "    timeline = train.loc[train[col2] != 'weoif', col1 + [col2]]\n",
    "    timeline['cnt'] = 1\n",
    "    timeline_se = timeline.groupby(col1 + [col2]).count()\n",
    "    timeline_se.reset_index(inplace =True)\n",
    "    temp2 = pd.merge(timeline_se, time_avg, how = 'left' , on=[col2])\n",
    "    temp2['multip'] = temp2['cnt'] * temp2[f'{col2}_avg']\n",
    "    temp2 = temp2.groupby(col1).sum().reset_index().drop([f'{col2}_avg'], axis =1)\n",
    "\n",
    "    temp2[f'{col2}_mean'] = temp2['multip'] / temp2['cnt']\n",
    "    temp2.drop(['multip','cnt'], axis=1 , inplace= True)\n",
    "\n",
    "    train= pd.merge(train, temp2, how ='left' , on=col1)\n",
    "    test= pd.merge(test, temp2, how ='left' , on=col1)\n",
    "    return train,test \n",
    "\n",
    "\n",
    "def create_grouped_features(train, test, group, numeric_var):\n",
    "    # 범주형 특성들에 대해서 다른 수치형 데이터의 중앙값, 최대, 합을 새로운 열로 추가하기 \n",
    "    train = train.copy()\n",
    "    test = test.copy()\n",
    "    aggs = ['median', 'max','sum']\n",
    "    for agg in aggs:\n",
    "        # groupby 후 aggregation\n",
    "        a1 = train.groupby([group])[numeric_var].agg(agg).to_dict()\n",
    "        # 새로운 feature 생성\n",
    "        train[numeric_var+'_'+group+'_'+agg] = train[group].map(a1)\n",
    "        test[numeric_var+'_'+group+'_'+agg] = test[group].map(a1)\n",
    "    return train, test\n",
    "\n",
    "def do_scale(train,test, scale_cols) :\n",
    "    for c in scale_cols:\n",
    "        min_value = train[c].min()\n",
    "        max_value = train[c].max()\n",
    "        train[c+'sc'] = (train[c] - min_value) / (max_value - min_value)\n",
    "        test[c+'sc'] = (test[c] - min_value) / (max_value - min_value)\n",
    "    return train,test\n",
    "\n",
    "class CFG:\n",
    "    user_seed = 42\n",
    "    target = 'is_converted'\n",
    "    \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG.user_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['business_unit','customer_idx']\n",
    "numeric_vars = ['historical_existing_cnt', 'lead_desc_length']\n",
    "scale_cols = ['com_reg_ver_win_rate','historical_existing_cnt', 'lead_desc_length','ver_win_rate_x'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 갖고오기 \n",
    "train,test= get_datas() \n",
    "\n",
    "# 스케일링 하기 \n",
    "train,test =do_scale(train,test,scale_cols)\n",
    "# 범주형 데이터에 대해 수치형 데이터 통계값 추가\n",
    "for group in groups:\n",
    "    for numeric_var in numeric_vars:\n",
    "        train, test = create_grouped_features(train, test, group, numeric_var)\n",
    "        \n",
    "        \n",
    "# 전처리, 로그변환 수행하기 \n",
    "columns_to_log=['com_reg_ver_win_rate','lead_desc_length']\n",
    "train,test= log_transform(train,columns_to_log ),log_transform(test,columns_to_log)\n",
    "train,test =eda_business_area(train),eda_business_area(test)\n",
    "train,test= get_nation_continent(train),get_nation_continent(test)\n",
    "train,test=eda_expected_timeline(train) ,eda_expected_timeline(test)\n",
    "train,test=customer_type(train) ,customer_type(test)\n",
    "train,test=eda_inquiry_type(train) ,eda_inquiry_type(test)\n",
    "\n",
    "# Feature Engineering \n",
    "train,test = fe_1(train,test)\n",
    "train,test = fe_2(train,test)\n",
    "train,test = fe_3(train,test)\n",
    "train,test = fe_9(train,test)\n",
    "train,test = fe_18(train,test, ['continent', 'bant_submit'],'inquiry_type')\n",
    "\n",
    "\n",
    "for col in ['customer_idx','customer_type',]:\n",
    "    train[col+'count'] =train[col].map(train[col].value_counts())\n",
    "    test[col+'count'] =test[col].map(train[col].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete=['nation']\n",
    "train,test =delete_cols(train, columns_to_delete), delete_cols(test,columns_to_delete)\n",
    "\n",
    "cols = [     'customer_country',    \"business_subarea\",    \"business_area\",    \"business_unit\",    \"customer_type\",    \"enterprise\",    \"customer_job\",    \"product_category\",    \"product_subcategory\",    \"product_modelname\",    \"customer_position\",\n",
    "      'customer_country.1', \"response_corporate\",  \n",
    "     \"expected_timeline\",'inquiry_type',\n",
    "'nation','continent',\n",
    "'total_area']\n",
    "label_columns =list(set(cols)-set(columns_to_delete))\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "enc = CatBoostEncoder(cols=label_columns)\n",
    "enc.fit(train[label_columns], train['is_converted'])  # 'target'은 실제 데이터의 타겟 변수 이름에 맞게 변경\n",
    "# 인코딩 적용\n",
    "train[label_columns] = enc.transform(train[label_columns])\n",
    "test[label_columns] = enc.transform(test[label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "x = train.drop('is_converted', axis= 1)\n",
    "y = train['is_converted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m avg_get_1 \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m322\u001b[39m,\u001b[38;5;241m8940\u001b[39m]:\n\u001b[0;32m---> 51\u001b[0m     _,f1,roc,ones,tts \u001b[38;5;241m=\u001b[39m\u001b[43mdtc_skfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     f1_avg\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m f1 \n\u001b[1;32m     53\u001b[0m     roc_avg\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m roc \n",
      "Cell \u001b[0;32mIn[147], line 10\u001b[0m, in \u001b[0;36mdtc_skfold\u001b[0;34m(zero_wei, one_wei, seed, x, y)\u001b[0m\n\u001b[1;32m      8\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      9\u001b[0m selector \u001b[38;5;241m=\u001b[39m RFECV(model, step\u001b[38;5;241m=\u001b[39mstep, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_features_to_select\u001b[38;5;241m=\u001b[39mmin_features_to_select)\n\u001b[0;32m---> 10\u001b[0m selector \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m x[features]\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:725\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    722\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    723\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m--> 725\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[1;32m    731\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:726\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    722\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    723\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m    725\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m--> 726\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[1;32m    728\u001b[0m )\n\u001b[1;32m    730\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[1;32m    731\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:37\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[0;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[1;32m     35\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[1;32m     36\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscores_\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py:326\u001b[0m, in \u001b[0;36mRFE._fit\u001b[0;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[1;32m    324\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(n_features)[support_]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_ \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Compute step score when only n_features_to_select features left\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_score:\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/aimers/lib/python3.9/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dtc_skfold(zero_wei,one_wei,seed,x,y) :\n",
    "     #Decisiontree에 대해서만 skfold 적용하는 함수 \n",
    "    real_preds = []\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    \n",
    "    model = DecisionTreeClassifier(random_state=seed ,class_weight =class_weight)\n",
    "    min_features_to_select = 25\n",
    "    step = 5\n",
    "    selector = RFECV(model, step=step, cv=10, min_features_to_select=min_features_to_select)\n",
    "    selector = selector.fit(x,y)\n",
    "    features = selector.get_feature_names_out()\n",
    "    x = x[features]\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    tt = []\n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test[features])\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.8533899999999999, 정밀도: 0.84992, 재현율: 0.8569100000000001, ROC-AUC: 0.9225880637864534\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.5   73.4]\n",
      " [  69.4  415.6]]\n",
      "> F1 Score: 0.8560800000000001, 정밀도: 0.8546999999999999, 재현율: 0.8575099999999999, ROC-AUC: 0.9232452978844942\n",
      "> 평균 검증 오차행렬: \n",
      " [[5374.2   70.7]\n",
      " [  69.1  415.9]]\n",
      "> F1 Score: 0.85656, 정밀도: 0.85365, 재현율: 0.8595900000000001, ROC-AUC: 0.9239972152451106\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.4   71.5]\n",
      " [  68.1  416.9]]\n",
      "> F1 Score: 0.85639, 정밀도: 0.8533000000000002, 재현율: 0.8595900000000001, ROC-AUC: 0.9241816513001189\n",
      "> 평균 검증 오차행렬: \n",
      " [[5373.2   71.7]\n",
      " [  68.1  416.9]]\n",
      "> F1 Score: 0.85341, 정밀도: 0.85022, 재현율: 0.85671, ROC-AUC: 0.9223988098257625\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.7   73.2]\n",
      " [  69.5  415.5]]\n",
      "> F1 Score: 0.8529499999999999, 정밀도: 0.8485699999999999, 재현율: 0.8575399999999999, ROC-AUC: 0.9228033309258882\n",
      "> 평균 검증 오차행렬: \n",
      " [[5370.6   74.3]\n",
      " [  69.1  415.9]]\n",
      "> F1 Score: 0.8588299999999999, 정밀도: 0.8587299999999999, 재현율: 0.85918, ROC-AUC: 0.9242631610185885\n",
      "> 평균 검증 오차행렬: \n",
      " [[5376.3   68.6]\n",
      " [  68.3  416.7]]\n",
      "> F1 Score: 0.85581, 정밀도: 0.85197, 재현율: 0.85978, ROC-AUC: 0.9238047374982239\n",
      "> 평균 검증 오차행렬: \n",
      " [[5372.4   72.5]\n",
      " [  68.   417. ]]\n",
      "> F1 Score: 0.8551399999999999, 정밀도: 0.84967, 재현율: 0.86083, ROC-AUC: 0.9242930589684679\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.    73.9]\n",
      " [  67.5  417.5]]\n",
      "> F1 Score: 0.85327, 정밀도: 0.8493200000000002, 재현율: 0.85734, ROC-AUC: 0.9227563820860007\n",
      "> 평균 검증 오차행렬: \n",
      " [[5371.1   73.8]\n",
      " [  69.2  415.8]]\n"
     ]
    }
   ],
   "source": [
    "seed_ens = []\n",
    "for seed in range(1,11):\n",
    "    dtc_preds,_,_,_,_= dtc_skfold(1,1,seed,x,y)\n",
    "    seed_ens.append(dtc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0: 3403\n",
      "Count of 1: 1868\n"
     ]
    }
   ],
   "source": [
    "predicts_array = np.array(seed_ens)\n",
    "average_preds = np.mean(predicts_array, axis=0)\n",
    "average_preds = np.sum(average_preds, axis =0 )\n",
    "final_prediction = np.where(average_preds < 0.6, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(final_prediction == 0))\n",
    "count_1 = np.size(np.where(final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 갖고오기 \n",
    "train,test= get_datas() \n",
    "\n",
    "# 스케일링 하기 \n",
    "train,test =do_scale(train,test,scale_cols)\n",
    "# 범주형 데이터에 대해 수치형 데이터 통계값 추가\n",
    "for group in groups:\n",
    "    for numeric_var in numeric_vars:\n",
    "        train, test = create_grouped_features(train, test, group, numeric_var)\n",
    "        \n",
    "        \n",
    "# 전처리, 로그변환 수행하기 \n",
    "columns_to_log=['com_reg_ver_win_rate','lead_desc_length']\n",
    "train,test= log_transform(train,columns_to_log ),log_transform(test,columns_to_log)\n",
    "train,test =eda_business_area(train),eda_business_area(test)\n",
    "train,test= get_nation_continent(train),get_nation_continent(test)\n",
    "train,test=eda_expected_timeline(train) ,eda_expected_timeline(test)\n",
    "train,test=customer_type(train) ,customer_type(test)\n",
    "train,test=eda_inquiry_type(train) ,eda_inquiry_type(test)\n",
    "\n",
    "# Feature Engineering \n",
    "train,test = fe_1(train,test)\n",
    "train,test = fe_2(train,test)\n",
    "train,test = fe_3(train,test)\n",
    "train,test = fe_9(train,test)\n",
    "train,test = fe_18(train,test, ['continent', 'bant_submit'],'inquiry_type')\n",
    "\n",
    "\n",
    "for col in ['customer_idx','customer_type',]:\n",
    "    train[col+'count'] =train[col].map(train[col].value_counts())\n",
    "    test[col+'count'] =test[col].map(train[col].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_delete=['nation']\n",
    "train,test =delete_cols(train, columns_to_delete), delete_cols(test,columns_to_delete)\n",
    "\n",
    "cols = [     'customer_country',    \"business_subarea\",    \"business_area\",    \"business_unit\",    \"customer_type\",    \"enterprise\",    \"customer_job\",    \"inquiry_type\",    \"product_category\",    \"product_subcategory\",    \"product_modelname\",    \"customer_position\",\n",
    "      'customer_country.1', \"response_corporate\",  \n",
    "     \"expected_timeline\",\n",
    "'nation','continent',\n",
    "'total_area',\n",
    "#         'res_unit'\n",
    "#         'idx_unit'\n",
    "# 'hemisphere'`\n",
    "      ]\n",
    "label_columns =list(set(cols)-set(columns_to_delete))\n",
    "\n",
    "from category_encoders import CatBoostEncoder\n",
    "enc = CatBoostEncoder(cols=label_columns)\n",
    "enc.fit(train[label_columns], train['is_converted'])  # 'target'은 실제 데이터의 타겟 변수 이름에 맞게 변경\n",
    "# 인코딩 적용\n",
    "train[label_columns] = enc.transform(train[label_columns])\n",
    "test[label_columns] = enc.transform(test[label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = train.columns\n",
    "vif['VIF'] = [variance_inflation_factor(train.values, i) for i in range(train.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "\n",
    "features_to_remove = vif.loc[vif['VIF'] > 10,'Features'].values\n",
    "features_to_remove = list(features_to_remove)\n",
    "train = train.drop(columns=features_to_remove, axis = 1)\n",
    "test = test.drop(columns=features_to_remove, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop('is_converted', axis=1)\n",
    "y = train.is_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.83422, 정밀도: 0.8252300000000001, 재현율: 0.8435, ROC-AUC: 0.9149326109968321\n",
      "> 평균 검증 오차행렬: \n",
      " [[5358.2   86.7]\n",
      " [  75.9  409.1]]\n",
      "> F1 Score: 0.83398, 정밀도: 0.8219199999999999, 재현율: 0.8465999999999999, ROC-AUC: 0.9161514397619145\n",
      "> 평균 검증 오차행렬: \n",
      " [[5355.8   89.1]\n",
      " [  74.4  410.6]]\n",
      "> F1 Score: 0.8339199999999998, 정밀도: 0.8229700000000001, 재현율: 0.84535, ROC-AUC: 0.9158188128726472\n",
      "> 평균 검증 오차행렬: \n",
      " [[5356.6   88.3]\n",
      " [  75.   410. ]]\n",
      "> F1 Score: 0.8346899999999999, 정밀도: 0.8248599999999999, 재현율: 0.84496, ROC-AUC: 0.915812278931047\n",
      "> 평균 검증 오차행렬: \n",
      " [[5357.8   87.1]\n",
      " [  75.2  409.8]]\n",
      "> F1 Score: 0.8318899999999999, 정밀도: 0.8213900000000001, 재현율: 0.8428800000000001, ROC-AUC: 0.9147159961235202\n",
      "> 평균 검증 오차행렬: \n",
      " [[5355.9   89. ]\n",
      " [  76.2  408.8]]\n",
      "0.8337399999999999 0.9154862277371922 409.66 5356.860000000001\n",
      "> F1 Score: 0.8288499999999999, 정밀도: 0.8175399999999999, 재현율: 0.84062, ROC-AUC: 0.9129778862063658\n",
      "> 평균 검증 오차행렬: \n",
      " [[5353.8   91.1]\n",
      " [  77.3  407.7]]\n",
      "Count of 0: 3434\n",
      "Count of 1: 1837\n"
     ]
    }
   ],
   "source": [
    "def dtc_skfold(zero_wei,one_wei,seed) :\n",
    "     #Decisiontree에 대해서만 skfold 적용하는 함수 \n",
    "    real_preds = []\n",
    "    class_weight={0:zero_wei , 1:one_wei}\n",
    "    model = DecisionTreeClassifier(random_state=seed ,class_weight =class_weight)\n",
    "    Skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  \n",
    "#     Skfold = StratifiedShuffleSplit(n_splits=10, random_state=42)  \n",
    "    cv_precision_scores, cv_recall_scores, cv_confusion_matrices, cv_f1_scores, cv_roc_auc_scores, cv_TN = [],[],[],[],[],[]\n",
    "    tt = []\n",
    "    for train_index, test_index in Skfold.split(x, y):  \n",
    "        x_train, x_test, y_train, y_test= x.iloc[train_index], x.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "        model.fit(x_train, y_train)\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred = model.predict(test)\n",
    "        real_preds.append(test_pred)\n",
    "        \n",
    "        pred_proba = model.predict_proba(x_test)[:, 1]  \n",
    "        \n",
    "        f1 = np.round(f1_score(y_test, pred, average='binary'), 4)  \n",
    "        precision = np.round(precision_score(y_test, pred, average='binary'), 4)  \n",
    "        recall = np.round(recall_score(y_test, pred, average='binary'), 4)  \n",
    "        conf_matrix = confusion_matrix(y_test, pred)  \n",
    "        roc_auc = roc_auc_score(y_test, pred_proba)  \n",
    "        \n",
    "        TN = conf_matrix[1][1]  # TN 값 저장\n",
    "        ttone = conf_matrix[0][0]\n",
    "        cv_TN.append(TN)  # TN 값 저장\n",
    "        tt.append(ttone)\n",
    "        cv_f1_scores.append(f1)  \n",
    "        cv_precision_scores.append(precision)  \n",
    "        cv_recall_scores.append(recall)  \n",
    "        cv_confusion_matrices.append(conf_matrix)  \n",
    "        cv_roc_auc_scores.append(roc_auc)  \n",
    "        \n",
    "    average_conf_matrix = np.mean(np.array(cv_confusion_matrices), axis=0)\n",
    "    print(f\"> F1 Score: {np.mean(cv_f1_scores)}, 정밀도: {np.mean(cv_precision_scores)}, 재현율: {np.mean(cv_recall_scores)}, ROC-AUC: {np.mean(cv_roc_auc_scores)}\")\n",
    "    print('> 평균 검증 오차행렬: \\n', average_conf_matrix)  \n",
    "    \n",
    "    return real_preds, np.mean(cv_f1_scores), np.mean(cv_roc_auc_scores), np.mean(cv_TN) ,np.mean(tt) # TN 평균 값 리턴\n",
    "\n",
    "f1_avg,roc_avg,tt=0,0,0\n",
    "avg_get_1 =0\n",
    "for seed in [5,11,30,322,8940]:\n",
    "    _,f1,roc,ones,tts =dtc_skfold(1,1,seed)\n",
    "    f1_avg+= f1 \n",
    "    roc_avg+= roc \n",
    "    avg_get_1+=ones\n",
    "    tt+= tts \n",
    "print(f1_avg/5,roc_avg/5,avg_get_1/5, tt/5)\n",
    "\n",
    "dtc_preds,_,_,_,_= dtc_skfold(1,1,3)\n",
    "\n",
    "predicts_array = np.array(dtc_preds)\n",
    "\n",
    "# axis=0를 기준으로 평균 계산\n",
    "# average_preds = np.mean(predicts_array, axis=0)\n",
    "\n",
    "# average_preds[0]\n",
    "final_prediction = np.mean(predicts_array, axis=0)\n",
    "\n",
    "final_prediction = np.where(final_prediction < 0.1, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(final_prediction == 0))\n",
    "count_2 = np.size(np.where(final_prediction >0))\n",
    "count_1 = np.size(np.where(final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.8318999999999999, 정밀도: 0.82301, 재현율: 0.84124, ROC-AUC: 0.9139012162762826\n",
      "> 평균 검증 오차행렬: \n",
      " [[5357.    87.9]\n",
      " [  77.   408. ]]\n",
      "> F1 Score: 0.83294, 정밀도: 0.8230999999999999, 재현율: 0.84329, ROC-AUC: 0.9145015541588604\n",
      "> 평균 검증 오차행렬: \n",
      " [[5356.8   88.1]\n",
      " [  76.   409. ]]\n",
      "> F1 Score: 0.8288499999999999, 정밀도: 0.8175399999999999, 재현율: 0.84062, ROC-AUC: 0.9129778862063658\n",
      "> 평균 검증 오차행렬: \n",
      " [[5353.8   91.1]\n",
      " [  77.3  407.7]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> F1 Score: 0.8340799999999999, 정밀도: 0.82607, 재현율: 0.8424699999999999, ROC-AUC: 0.9144676095468769\n",
      "> 평균 검증 오차행렬: \n",
      " [[5358.7   86.2]\n",
      " [  76.4  408.6]]\n",
      "> F1 Score: 0.83422, 정밀도: 0.8252300000000001, 재현율: 0.8435, ROC-AUC: 0.9149326109968321\n",
      "> 평균 검증 오차행렬: \n",
      " [[5358.2   86.7]\n",
      " [  75.9  409.1]]\n",
      "> F1 Score: 0.8322900000000001, 정밀도: 0.82269, 재현율: 0.8424799999999999, ROC-AUC: 0.9144730745817004\n",
      "> 평균 검증 오차행렬: \n",
      " [[5356.6   88.3]\n",
      " [  76.4  408.6]]\n",
      "> F1 Score: 0.83256, 정밀도: 0.8231999999999999, 재현율: 0.8422699999999999, ROC-AUC: 0.9145118437803385\n",
      "> 평균 검증 오차행렬: \n",
      " [[5357.1   87.8]\n",
      " [  76.5  408.5]]\n",
      "> F1 Score: 0.83246, 정밀도: 0.8280800000000001, 재현율: 0.8371299999999999, ROC-AUC: 0.9120443762136736\n",
      "> 평균 검증 오차행렬: \n",
      " [[5360.5   84.4]\n",
      " [  79.   406. ]]\n",
      "> F1 Score: 0.83491, 정밀도: 0.82761, 재현율: 0.8424700000000002, ROC-AUC: 0.9143564095624965\n",
      "> 평균 검증 오차행렬: \n",
      " [[5359.7   85.2]\n",
      " [  76.4  408.6]]\n",
      "> F1 Score: 0.83453, 정밀도: 0.82629, 재현율: 0.84331, ROC-AUC: 0.9155778514187098\n",
      "> 평균 검증 오차행렬: \n",
      " [[5358.7   86.2]\n",
      " [  76.   409. ]]\n"
     ]
    }
   ],
   "source": [
    "# seed 앙상블 (현재는 1부터 10 )\n",
    "\n",
    "seed_ens2 = []\n",
    "for seed in range(1,11):\n",
    "    dtc_preds,_,_,_,_= dtc_skfold(1,1,seed)\n",
    "    seed_ens2.append(dtc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5271,)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts_array = np.array(seed_ens2)\n",
    "average_preds = np.mean(predicts_array, axis=0)\n",
    "average_preds = np.sum(average_preds, axis =0 )\n",
    "average_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0: 3269\n",
      "Count of 1: 2002\n"
     ]
    }
   ],
   "source": [
    "predicts_array = np.array(seed_ens)\n",
    "average_preds = np.mean(predicts_array, axis=0)\n",
    "average_preds = np.sum(average_preds, axis =0 )\n",
    "\n",
    "predicts_array2 = np.array(seed_ens2)\n",
    "average_preds2 = np.mean(predicts_array2, axis=0)\n",
    "average_preds2 = np.sum(average_preds2, axis =0 ) # 이건 평균이 아니라 SUM 이네\n",
    "\n",
    "average_preds_total = (average_preds + average_preds2)/2\n",
    "\n",
    "final_prediction = np.where(average_preds_total < 0.6, 0, 1)\n",
    "\n",
    "count_0 = np.size(np.where(final_prediction == 0))\n",
    "count_1 = np.size(np.where(final_prediction == 1))\n",
    "\n",
    "# 각 값을 출력\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)\n",
    "\n",
    "sub=pd.read_csv('submission.csv')\n",
    "sub['is_converted']= final_prediction\n",
    "sub.to_csv('submission.csv',index= False)\n",
    "sub.to_csv('REFCV_VIF_0.6_final.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/workspace/LGamiers/Tunning/REFCV_VIF_0.6_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'bant_submit', 'customer_country', 'business_unit',\n",
       "       'com_reg_ver_win_rate', 'customer_idx', 'customer_type', 'enterprise',\n",
       "       'historical_existing_cnt', 'id_strategic_ver', 'it_strategic_ver',\n",
       "       'idit_strategic_ver', 'customer_job', 'lead_desc_length',\n",
       "       'inquiry_type', 'product_category', 'product_subcategory',\n",
       "       'product_modelname', 'customer_country.1', 'customer_position',\n",
       "       'response_corporate', 'expected_timeline', 'ver_cus', 'ver_pro',\n",
       "       'ver_win_rate_x', 'ver_win_ratio_per_bu', 'business_area',\n",
       "       'business_subarea', 'lead_owner', 'is_converted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selector.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
